\documentclass[UTF8,a4paper,10pt]{article}

\input{preamble.tex}

% \begin{equation*}
%   \begin{aligned}
%   \end{aligned}
% \end{equation*}

% \begin{mybox}{}
% \end{mybox}


% \begin{Problem}[]{}
% \end{Problem}

% \begin{solution}\,
% \end{solution}
  

% \begin{enumerate}[label=(\alph*)]
% \end{enumerate} 

% \setcounter{section}{3} 
% \setcounter{theorem}{3}

% \begin{theorem}\label{thm:3.4}
%   If $E = \bigcup_{k} E_k$ is a countable union of sets, then $|E|_e \leq \sum_{k} |E_k|_e$.
%   \end{theorem}

%  \footcite[][42]{Wheeden_Zygmund_2015}

\begin{document}

Thanks to D11221003


\begin{mybox}{Exponential Family}
  % \section{}
  \begin{itemize}
      \item Suppose $Y_1, \ldots, Y_n$ are independent random variables.
      \item Let $f(y_i; \theta_i, \phi)$ be the Probability Mass Function (PMF) or Probability Density Function (PDF) of $Y_i$, where $\phi$ is a scale parameter.
      \item If we can write
      \[
      f(y_i; \theta_i, \phi) = \exp\left(y_i \theta_i - b(\theta_i) \frac{1}{a(\phi)} + c(y_i, \phi)\right),
      \]
      then we call the PMF or the PDF $f(y_i; \theta_i, \phi)$ an exponential family.
  \end{itemize}
  


\end{mybox}


% Q1


  \begin{Problem}[]{}
    Find the form of GLM for the following distributions, and show the resonable link function:
    \begin{enumerate}
      \item Normal distributions
      \item Inverse Gaussian
      \item Binomial distribution
      \item Poisson distribution
      \item Gamma distribution
      \item Beta 
      % (重新參數化)
    \end{enumerate}
    
  \end{Problem}


  \begin{mybox}{Normal Distribution}
  % \section{Normal Distribution}
Assume $Y_i \sim \mathcal{N}(\mu_i, \sigma^2)$. Then, $E(Y_i) = \mu_i$ and $\sigma$ is a scale parameter. The Probability Density Function (PDF) is given by
\[
\frac{1}{\sqrt{2\pi\sigma}}e^{-\frac{(y_i - \mu_i)^2}{2\sigma^2}} = \exp\left\{\frac{y_i\mu_i - \mu_i^2/2}{\sigma^2} - \left(\frac{1}{2}\log(2\pi\sigma^2) - \frac{y_i^2}{2\sigma^2}\right)\right\}.
\]

Thus, use 

\begin{itemize}
  \item $\theta_i = \mu_i$,
  \item $b(\theta_i) = \frac{\theta_i^2}{2}$,
  \item $\phi = \sigma^2$, 
  \item $a(\phi) = \phi$,
  \item $c(y_i, \phi) = -\frac{1}{2}\log(2\pi\phi) - \frac{y_i^2}{2\phi}$.
\end{itemize}

\[E[Y_{ij}\,|\, x_{ij}] = h(x_{ij}^T\beta) = x_{ij}^T\beta\]


\end{mybox}
\pagebreak


\begin{mybox}{Inverse Gaussian Distribution}
  Let us rewrite the probability density function (pdf) of the Inverse Gaussian distribution with parameters $\mu_i$ and $\lambda$:
  \[
  f(y_i; \mu_i, \lambda) = \left(\frac{\lambda}{2\pi y_i^3}\right)^{1/2}\exp\left\{-\frac{\lambda(y_i - \mu_i)^2}{2\mu_i^2 y_i}\right\}, \quad y > 0
  \]
  
  in the following form:
  \[
  \begin{aligned}
  f(y_i; \mu_i, \lambda) &= \exp\left\{-\frac{\lambda(y_i - \mu_i)^2}{2\mu_i^2 y_i} + \frac{1}{2}\ln\left(\frac{\lambda}{2\pi y_i^3}\right)\right\} \\
  &=\exp\left\{\frac{-\frac{1}{2\mu_i^2}y_i+\frac{1}{\mu_i}}{\frac{1}{\lambda}}+\left(\frac{1}{2}\ln \frac{\lambda}{2\pi y_i^3}-\frac{\lambda}{2y_i}\right) \right\}
  \end{aligned}
  \]
  
  Now, let's identify the exponential family components:
  \begin{itemize}
      \item Canonical parameter: $\theta_i = -\frac{1}{2\mu_i^2}$
      \item $b(\theta_i) = -\frac{1}{\mu_i} = -(-2\theta_i)^{\frac{1}{2}} $
      \item $\phi = \lambda$
      \item \(a(\phi) = \frac{1}{\phi}\)
      \item \(c(y_i,\phi) = \frac{1}{2}\ln\frac{\phi}{2\pi y_i^{3}}-\frac{\phi}{2y_i}\)
  \end{itemize}
  
  Thus, the Inverse Gaussian distribution can be shown to be a member of the exponential family.

  \[E[Y_{ij}\,|\, x_{ij}] = h(x_{ij}^T\beta) = \exp(x_{ij}^T\beta)>0\]

\end{mybox}
  
  
\begin{mybox}{Binomial Distribution}
  % \section{}
Assume $Y_i \sim \text{Bin}(n_i, p_i)$. Then, $E(Y_i) = n_i p_i$. The Probability Mass Function (PMF) is given by
\[
\binom{n_i}{y_i} p_i^{y_i} (1 - p_i)^{n_i - y_i} = \exp\left\{y_i \log \left(\frac{p_i}{1 - p_i}\right) + n_i \log(1 - p_i) - \log \binom{n_i}{y_i}\right\}.
\]

Thus, 
\begin{itemize}
  \item $\theta_i = \log\left(\frac{p_i}{1 - p_i}\right)$, 
  \item $b(\theta_i) = n_i \log(1 + e^{\theta_i})$, 
  \item $\phi = 1$, $a(\phi) = 1$,
  \item $c(y, \phi) = -\log \binom{n_i}{y_i}$.
\end{itemize}

\[E[Y_{ij}\,|\, x_{ij}] = h(x_{ij}^T\beta) = n \frac{\exp(x_{ij}^T\beta)}{1+\exp(x_{ij}^T\beta)}\in (0,n)\]

\end{mybox}

\pagebreak
  
\begin{mybox}{Poisson Distribution}
  % \section{}
Assume $Y_i \sim \text{Poisson}(\lambda_i)$. Then, $E(Y_i) = \lambda_i$. The Probability Mass Function (PMF) is given by
\[
\frac{\lambda_i^{y_i}e^{-\lambda_i}}{y_i!} = \exp\left\{y_i\log(\lambda_i) - \lambda_i - \log(y_i!)\right\}.
\]

Thus, 
\begin{itemize}
  \item $\theta_i = \log(\lambda_i)$, 
  \item $b(\theta_i) = e^{\theta_i}$, 
  \item $\phi = 1$, $a(\phi) = 1$, 
  \item $c(y_i, \phi) = -\log(y_i!)$.
\end{itemize}

\[E[Y_{ij}\,|\, x_{ij}] = h(x_{ij}^T\beta) = \exp(x_{ij}^T\beta)>0\]


\end{mybox}

  
\begin{mybox}{Gamma Distribution}
  % \section{}
Assume $x_i \sim \Gamma(\alpha, \beta_i)$, where $\beta_i$ is unknown. Then, $E(x_i) = \frac{\alpha}{\beta_i}$. The Probability Mass Function (PMF) is given by
\[
\frac{\beta_i^\alpha x_i^{\alpha-1} e^{-\beta_i x_i}}{\Gamma(\alpha)} = \exp\left\{\alpha\log x_i + \alpha\log(\beta_i) - \log(\Gamma(\alpha)) - \log(x_i) - \beta_i x_i\right\}.
\]

Assuming $\alpha$ is known, if we choose $y_i = x_i$, then 
\begin{itemize}
  \item $\theta_i = -\beta_i$ ($\theta_i < 0$), 
  \item $b(\theta_i) = -\alpha\log(-\theta_i)$, 
  \item $\phi = 1$, and $a(\phi) = 1$.
\end{itemize}

Remark: We can also choose 
$y_i = -x_i$ and $\theta_i = \beta_i$. In this case, $b(\theta_i) = -\alpha\log(\theta_i)$.

\[E[Y_{ij}\,|\, x_{ij}] = h(x_{ij}^T\beta) = \exp(x_{ij}^T\beta)>0\]


\end{mybox}

% \begin{mybox}{Beta Distribution}
% % \section{}
% Assume $x_i \sim \text{Beta}(\alpha, \beta_i)$, where $\beta_i$ is unknown. Then, the expected value of $x_i$ is $E(x_i) = \frac{\alpha}{\alpha + \beta_i}$. The Probability Mass Function (PMF) is given by

% \begin{equation*}
%   \begin{aligned}
% &\frac{\Gamma(\alpha + \beta_i)}{\Gamma(\alpha)\Gamma(\beta_i)} x_i^{\alpha - 1}(1 - x_i)^{\beta_i - 1}\\ 
% = &\exp\left\{\log(\Gamma(\alpha + \beta_i)) - \log(\Gamma(\alpha)) - \log(\Gamma(\beta_i)) + (\alpha - 1)\log(x_i) + (\beta_i - 1)\log(1 - x_i)\right\}.
% \end{aligned}
% \end{equation*}

% Assuming $\alpha$ is known, if we choose $y_i = x_i$, then $\theta_i = \beta_i$, $b(\theta_i) = -\log(\theta_i)$, $\phi = 1$, and $a(\phi) = 1$.

% Remark: We can also choose $y_i = 1 - x_i$ and $\theta_i = \alpha$. In this case, $b(\theta_i) = -\log(\theta_i)$.
% \end{mybox}


\pagebreak

  \begin{Problem}[]{}
  Paper Summarization for "Sliced Inverse Regression for Dimension Reduction"\footnote{Li, Ker-Chau. “Sliced Inverse Regression for Dimension Reduction.” Journal of the American Statistical Association 86, no. 414 (1991): 316–27. https://doi.org/10.2307/2290563.}
    
  \end{Problem}

\begin{enumerate}
  \item The paper proposes a data-analytic tool, sliced inverse regression (SIR), for reducing the dimension of the input variable \(x\) without going through any parametric or nonparametric model-fitting process.
  \item The paper explores the inverse view of regression, that is, instead of regressing the output variable \(y\) against the input variable \(x\), it regresses \(x\) against \(y\). The paper shows that under a suitable condition, the inverse regression curve \(E(x \,|\, y)\) will fall into the effective dimension reduction (e.d.r.) space, which is the linear space generated by the unknown row vectors \(\beta\) in the model \(y = f(\beta_1 x, \cdots, \beta_k x, \epsilon)\).
  \item The paper develops a simple algorithm for SIR, which consists of standardizing \(x\), slicing the range of \(y\) into several intervals, computing the slice means of \(x\) within each interval, and conducting a principal component analysis on the covariance matrix of the slice means to estimate the e.d.r. directions.
  \item The paper investigates the theoretical properties of SIR under a design condition that is satisfied by elliptically symmetric distributions. The paper shows that SIR yields \(\sqrt{n}-\) consistent estimates for the e.d.r. directions and provides a chi-squared statistic to test whether an estimated direction is spurious.
  % \item The paper demonstrates the effectiveness of SIR by simulation and compares it with other methods such as projection pursuit regression and partial spline models. The paper also illustrates how SIR can be used as a graphical tool to view high-dimensional data and as a variable selection tool to find variables that are most predictable from \(y\).
\end{enumerate}

---

Reference:\footnote{Wikipedia contributors, "Sliced inverse regression," Wikipedia, The Free Encyclopedia, \url{https://en.wikipedia.org/wiki/Sliced_inverse_regression} (accessed October 23, 2023).}


\pagebreak

  \begin{Problem}[]{}
    Binary response (\(Y_{ij}=0/1\))

    Logistic regression model:
\begin{enumerate}
  \item \(\E[Y_{ij}\,|\,x_{ij}] = h(x_{ij}^T\beta) = \frac{\exp(x_{ij}^T\beta)}{1+\exp(x_{ij}^T\beta)}\)
  \item \(\operatorname{Var}[Y_{ij}\,|\,x_{ij}] = \E[Y_{ij}\,|\,x_{ij}](1-\E[Y_{ij}\,|\,x_{ij}])\)
  \item \(\Cor[Y_{ij},Y_{ik}\,|\,x_{ij},x_{ik}]= \alpha\) 
  \item odd ratio: \(OR(Y_{ij},Y_{ik}) = \frac{P(Y_{ij} = 1,Y_{ik} = 1)P(Y_{ij} = 0,Y_{ik} = 0)}{P(Y_{ij} = 1,Y_{ik} = 0)P(Y_{ij} = 0,Y_{ik} = 1)}\)
\end{enumerate}
    
  \end{Problem}

  % Under the assumption that \(\)

\begin{equation*}
  \begin{aligned}
    \alpha = \Cor[Y_{ij},Y_{ik}\,|\,x_{ij},x_{ij}] 
    &= \frac{\E[(Y_{ij}-\E[Y_{ij}\,|\,x_{ij}])(Y_{ik}-\E[Y_{ik}\,|\,x_{ik}])]}{\sigma_{Y_{ij}\,|\,x_{ij}}\sigma_{Y_{ik}\,|\,x_{ik}}}\\
    &=\frac{\E[(Y_{ij}-\E[Y_{ij}\,|\,x_{ij}])(Y_{ik}-\E[Y_{ik}\,|\,x_{ik}])]}{\sqrt{\operatorname{Var}[Y_{ij}\,|\,x_{ij}]}\sqrt{\operatorname{Var}[Y_{ik}\,|\,x_{ik}] }}\\
    &=\frac{\E[(Y_{ij}-\E[Y_{ij}\,|\,x_{ij}])(Y_{ik}-\E[Y_{ik}\,|\,x_{ik}])]}{\sqrt{\E[Y_{ij}\,|\,x_{ij}](1-\E[Y_{ij}\,|\,x_{ij}])}\sqrt{\E[Y_{ik}\,|\,x_{ik}](1-\E[Y_{ik}\,|\,x_{ik}])}}\\
    &=\frac{\E[Y_{ij}Y_{ik}-Y_{ij}h(x_{ik}^T\beta)-Y_{ik}h(x_{ik}^T\beta)+h(x_{ij}^T\beta)h(x_{ik}^T\beta)]}{\sqrt{h(x_{ij}^T\beta)(1-h(x_{ij}^T\beta))}\sqrt{h(x_{ik}^T\beta)(1-h(x_{ik}^T\beta))}}\\
    &=\frac{\E[Y_{ij}Y_{ik}]-h(x_{ij}^T\beta)h(x_{ik}^T\beta)}{\sqrt{h(x_{ij}^T\beta)(1-h(x_{ij}^T\beta))}\sqrt{h(x_{ik}^T\beta)(1-h(x_{ik}^T\beta))}}\\
    &=\frac{P(Y_{ij} = 1,Y_{ik} = 1)-h(x_{ij}^T\beta)h(x_{ik}^T\beta)}{\sqrt{h(x_{ij}^T\beta)(1-h(x_{ij}^T\beta))}\sqrt{h(x_{ik}^T\beta)(1-h(x_{ik}^T\beta))}}\in[0,1]
  \end{aligned}
\end{equation*}
\dotfill

Denoting \(P_{ij,ik} = P(Y_{ij} = 1,Y_{ik} = 1).\)


\begin{equation*}
  \begin{aligned}
    \gamma = OR(Y_{ij},Y_{ik}) &= \frac{P(Y_{ij} = 1,Y_{ik} = 1)P(Y_{ij} = 0,Y_{ik} = 0)}{P(Y_{ij} = 1,Y_{ik} = 0)P(Y_{ij} = 0,Y_{ik} = 1)}\\
    &= \frac{P_{ij,ik}[1-(h(x_{ij}^T\beta)-P_{ij,ik})-(h(x_{ik}^T\beta)-P_{ij,ik})-P_{ij,ik}]}{[h(x_{ij}^T\beta) -P_{ij,ik}][h(x_{ik}^T\beta)-P_{ij,ik}]}\\
    &= \frac{P_{ij,ik}[1-h(x_{ij}^T\beta)-h(x_{ik}^T\beta)+P_{ij,ik}]}{[h(x_{ij}^T\beta) -P_{ij,ik}][h(x_{ik}^T\beta)-P_{ij,ik}]}\\
    % P_{ij,ik} = P(Y_{ij} = 1,Y_{ik} = 1) &= \frac{\gamma P(Y_{ij} = 1,Y_{ik} = 0)P(Y_{ij} = 0,Y_{ik} = 1)}{P(Y_{ij} = 0,Y_{ik} = 0)}\\
    % & = \frac{\gamma P(Y_{ij} = 1,Y_{ik} = 0)P(Y_{ij} = 0,Y_{ik} = 1)}{P(Y_{ij} = 0,Y_{ik} = 0)}\\
    % & = \frac{\gamma }{}
  \end{aligned}
\end{equation*}

Solve for \(P_{ij,ik}\)
  using \(\gamma\) and \(h(\cdot)\).

\pagebreak
\begin{Problem}[]{}
  Describe
  \begin{enumerate}
    \item how to conduct the EM (Expectation-Maximization) algorithm
    \item how to conduct  MCMC
  \end{enumerate}
    
  \end{Problem}
 
  EM

  Denoting
  \[ Q(q|q_0, x) = \mathbb{E}_{q_0} \left[ \log L_c(q|x,Z) \right], \]
  the EM algorithm indeed proceeds "iteratively" by maximizing \(Q(q|q_0, x)\) at each iteration, and, if \(q^{\hat{(1)}}\) is the value of \(q\) maximizing \(Q(q|q_0, x)\), by replacing \(q_0\) by the updated value \(q^{\hat{(1)}}\). In this manner, a sequence of estimators \(\{q^{\hat{(j)}}\}_j\) is obtained, where
  \[ Q(q^{\hat{(j)}}|q^{\hat{(j-1)}}) \]

  Pick a starting value \(q^{\hat{(0)}}\) and set \(m = 0\).
Repeat
\begin{enumerate}
  \item Compute (the E-step) \[ Q(q|q^{\hat{(m)}, x}) = \mathbb{E}_{q^{\hat{(m)}}} \left[ \log L_c(q|x,Z) \right], \]where the expectation is with respect to \(k(z|q^{\hat{(m)}, x})\).
  \item Maximize \(Q(q|q^{\hat{(m)}, x})\) in \(q\) and take (the M-step)   \[ q^{\hat{(m+1)}} = \arg\max_q Q(q|q^{\hat{(m)}, x}) \] and set \(m = m+1\).
\end{enumerate}

until a fixed point is reached; i.e., \(q^{\hat{(m+1)}} = q^{\hat{(m)}}\).

  

\dotfill

Gibbs Sampling (Markov Chain Monte Carlo)



Consider $f\left(y_{i j} \mid u_{i}, \beta\right)=e^{\left(\frac{y_{i j} \theta_{i j}-\psi\left(\theta_{i j}\right)}{a(\phi)}-c\left(y_{i j} ; \phi\right)\right)}$ 

with $g\left(u_{i} \mid G\right)=(2 \pi)^{\frac{-q}{2}}|G|^{\frac{-1}{2}} e^{\frac{-u_{i}^{T} G^{-1} u_{i}}{2}}$, and $h\left(\mu_{i j}\right)=x_{i j}^{T} \beta+z_{i j}^{T} u_{i}$.

The likelihood function of $(\beta, G)$ is

\[L(\beta, G\,|\,  \underset{\sim}{y})\propto \prod_{i=1}^n\prod_{j=1}^{m_i}f(y_{ij}\,|\,u_i,\beta)|G|^{-1/2}\exp\{\frac{u_i^TG^{-1}u_i}{2}\}du_i. \]

% ![](https://cdn.mathpix.com/cropped/2023_10_23_545ee9d39b59250db51fg-6.jpg?height=117&width=1071&top_left_y=2437&top_left_x=298)

In a Bayesian approach to analyzing the random effects $\operatorname{GLM}$, the parameters $(\beta, G)$ are random variables and are treated symmetrically with the longitudinal measurements and unobserved latent variables. Thus, the random effects GLM is an example of a hierarchical Bayes model.

Assumptions: $[\beta \mid G, U, \underset{\sim}{y}]=[\beta \mid U, \underset{\sim}{y}],[G \mid \beta, U, \underset{\sim}{y}]=[G \mid U]$ and $[U \mid \beta, G, \underset{\sim}{y}]$.

1. Assume that $\beta$ has a flat prior function. Then,

$\left[\beta \mid U^{(k)}, \underset{\sim}{y}\right] \propto \prod_{i=1}^{n} \prod_{j=1}^{m_{i}} f\left(y_{i j} \mid U_{i}^{(k)}, \beta\right) \approx N\left(\beta^{(k)}, V_{\beta}^{(k)}\right)$, as $n \rightarrow \infty$, where $\beta^{(k)}$ is the maximum likelihood estimator and $V_{\beta}^{(k)}$ is the inverse of the Fisher information.

Adjustment for smaller samples - "Rejection sampling" (Ripley, 1987)

Let $f\left(\beta \mid U^{(k)}, \underset{\sim}{y}\right)$ and $\phi\left(\beta \mid \beta^{(k)}, V_{\beta}^{(k)}\right)$ denote separately the true density and Gaussian density. Choose a constant $c \geq 1$ such that $c \phi\left(\beta \mid \beta^{(k)}, V_{\beta}^{(k)}\right) \geq f\left(\beta \mid U^{(k)}, \underset{\sim}{y}\right)$.

Step1: Generate $\beta^{*} \sim \phi\left(\beta \mid \beta^{(k)}, V_{\beta}^{(k)}\right)$ and $u \sim U(0,1)$.

Step2: If $\frac{f\left(\beta^{*} \mid b^{(k)}, \underset{\sim}{y}\right)}{c \phi\left(\beta^{*} \mid \beta^{(k)}, V_{\beta}^{(k)}\right)}<u, \beta^{(k+1)}=\beta^{*}$. Otherwise, the process returns to Step1.

2. $\left[G \mid U^{(k)}\right]$

Assume that $\pi(G) \propto|G|^{-1}$: non-informative prior (see Box and Tiao, 1973). 

Then, $\left[G \mid U^{(k)}\right] \sim \operatorname{Inverted}$ Wishart $\left(S^{(k)}, n-q+1\right)$, where $S^{(k)}=\sum_{i=1}^{n} U_{i}^{(k)} U_{i}^{(k)^{T}}$
\\

Remark

If $\mathrm{A} \sim \mathrm{Wishart}\left(\Sigma_{p \times p}, n\right)$, the p.d.f of $\mathrm{A} $ is $f_{\mathrm{A}}(\mathrm{A}) \propto|\mathrm{A}|^{\frac{-1}{2}(n-p-1)} e^{\frac{-1}{2} tr \Sigma^{-1} \mathrm{A}}$. 

It implies that $\mathrm{B}=\mathrm{A}^{-1} \sim \operatorname{Inverted}$ Wishart $\left(\Sigma^{-1}, n\right)$ with p.d.f. $f_{\mathrm{B}}(\mathrm{B}) \propto|\mathrm{B}|^{\frac{-1}{2}(n+p+1)} e^{\frac{-1}{2} t r \Sigma^{-1} \mathrm{~B}^{-1}}$. 

Thus, 

\[\pi\left(G \mid U^{(k)}\right) \propto|G|^{\frac{-1}{2}(n+2)} e^{\frac{-1}{2} \operatorname{tr}\left(S^{(k)} G^{-1}\right)},\] 
i.e. $\left[G \mid U^{(k)}\right] \sim \operatorname{Inverted~Wishart}\left(S^{(k)}, n-q+1\right)$.
\\

3. $\left[U \mid \beta^{(k)}, G^{(k)}, y\right]$

Using 
\[f\left(U_{i} \mid \hat{\beta}^{(k)}, G^{(k)},\underset{\sim}{y_{i}}\right) \propto f\left(\underset{\sim}{y_{i}} \mid U_{i}, \hat{\beta}^{(k)}\right) g\left(U_{i} \mid G^{(k)}\right) \triangleq f_{n}\left(U_{i} \mid \hat{\beta}^{(k)}, G^{(k)},\underset{\sim}{y_{i}}\right),\] 
we can find the mode and curvature of $f_{n}\left(U_{i} \mid \hat{\beta}^{(k)}, G^{(k)},\underset{\sim}{y_{i}}\right)$, which matches a Gaussian density.

Using the surrogate response \[Z_{i}^{*}=X_{i} \beta+D_{i} U_{i}+\operatorname{Diag}\left(h^{\prime}\left(\mu_{i}\right)\right)\left(y_{i}-\mu_{i}\right),\] 

the maximum value of $f_{n}\left(U_{i} \mid \hat{\beta}^{(k)}, G^{(k)}, \underset{\sim}{y_{i}}\right)$ occurs at 
\[U_{i}=\left(D_{i}^{T} Q_{i}^{-1} D_{i}+G^{(k)^{-1}}\right)^{-1} D_{i}^{T} Q_{i}^{-1}\left(Z_{i}^{*}-X_{i} \beta^{(k)}\right)=G^{(k)} D_{i}\left(D_{i} G^{(k)} D_{i}^{T}+Q_{i}\right)^{-1}\left(Z_{i}^{*}-X_{i} \hat{\beta}^{(k)}\right)\]
and its curvature is $V_{i}=\left(D_{i}^{T} Q_{i}^{-1} D_{i}+G^{(k)^{-1}}\right)^{-1}$. Similar to the method in (3), $U_{i}^{(k)}$ can be obtained.

\pagebreak

Generalized Estimating Equations (GEE), which is a multivariate analogue of
quasi-likelihood.

% Generalized Estimating Equations (GEE):

$S_{\beta}(\beta, \alpha)=\sum_{i=1}^{n}\left(\dfrac{\partial \mu_{i}}{\partial \beta}\right)^{T}\left[\operatorname{Var}\left(Y_{i}\right)\right]^{-1}\left(Y_{i}-\mu_{i}\right)$, where $\mu_{i}=h\left(x_{i j}^{T} \beta\right), \operatorname{Var}\left(Y_{i}\right)=\operatorname{Var}\left(Y_{i} ; \beta, \alpha\right)$

$S_{\alpha}(\beta, \alpha)=\sum_{i=1}^{n}\left(\dfrac{\partial \eta_{i}}{\partial \alpha}\right)^{T} H_{i}^{-1}\left(\omega_{i}-\eta_{i}\right),$ where $\omega_{i}=\left(R_{i 1} R_{i 2}, \cdots, R_{i 1} R_{i m_{i}}, \cdots, R_{i 1}^{2}, \cdots, R_{i m_{i}}^{2}\right)$

\(\eta_i = \E[\omega_i\,|\,(\beta,\alpha)]\), and \(H_i = \operatorname{Var}(\omega_i)\), with \(R_{ij} = \frac{Y_{ij} - \mu_{ij}}{\sqrt{\operatorname{Var}(Y_{ij})}}\)

The estimator, say $(\hat{\beta}, \hat{\alpha})$ of $(\beta, \alpha)$ is defined to be the solution of the above equations,

i.e. $S_{\beta}(\hat{\beta}, \hat{\alpha})=0$ and $S_{\alpha}(\hat{\beta}, \hat{\alpha})=0$.

\begin{Problem}[]{}
  Theorem 3.1. 
  
  Under the regularity conditions, $n^{\frac{1}{2}}\left[\left(\begin{array}{l}\hat{\beta} \\ \hat{\alpha}\end{array}\right)-\left(\begin{array}{l}\beta \\ \alpha\end{array}\right)\right] \stackrel{d}{\rightarrow} N(0, \Sigma)$, where $\Sigma$ can be estimated by 
  \[\left(\frac{1}{n} \sum_{i=1}^{n} C_{i}^{T} B_{i}^{-1} D_{i}\right)^{-1}\left(\frac{1}{n} \sum_{i=1}^{n} C_{i}^{T} B_{i}^{-1} V_{0 i} B_{i}^{-1} C_{i}\right)\left(\frac{1}{n} \sum_{i=1}^{n} D_{i}^{T} B_{i}^{-1} C_{i}\right)^{-1},\]

  where $C_{i}=\left(\begin{array}{cc}\dfrac{\partial \mu_{i}}{\partial \beta} & 0 \\ 0 & \dfrac{\partial \eta_{i}}{\partial \alpha}\end{array}\right), B_{i}=\left(\begin{array}{cc}\operatorname{Var}\left(Y_{i}\right) & 0 \\ 0 & H_{i}\end{array}\right), D_{i}=\left(\begin{array}{cc}\dfrac{\partial \mu_{i}}{\partial \beta} & \dfrac{\partial \mu_{i}}{\partial \alpha} \\ \dfrac{\partial \eta_{i}}{\partial \beta} & \dfrac{\partial \eta_{i}}{\partial \alpha}\end{array}\right)$, 
  
  and $V_{0 i}=\left(\begin{array}{c}y_{i}-\mu_{i} \\ \omega_{i}-\eta_{i}\end{array}\right)^{\otimes 2}$.

  ---
  
Hint

\begin{equation*}
  \begin{aligned}
\left(\begin{array}{l}0 \\ 0\end{array}\right)
&=\left(\begin{array}{l}S_{\beta}(\hat{\beta}, \hat{\alpha}) \\ S_{\alpha}(\hat{\beta}, \hat{\alpha})\end{array}\right)\\
&=\left(\begin{array}{l}S_{\beta}(\beta, \alpha) \\ S_{\alpha}(\beta, \alpha)\end{array}\right)+\left.\left(\begin{array}{ll}\dfrac{\partial S_{\beta}(\beta, \alpha)}{\partial \beta} & \dfrac{\partial S_{\beta}(\beta, \alpha)}{\partial \alpha} \\ \dfrac{\partial S_{\alpha}(\beta, \alpha)}{\partial \beta} & \dfrac{\partial S_{\alpha}(\beta, \alpha)}{\partial \alpha}\end{array}\right)\right|_{\begin{pmatrix}\left.\hat{\beta}^{*}\right. \\ \hat{\alpha}^{*}\end{pmatrix}}
\left[\left(\begin{array}{l}\hat{\beta} \\ \hat{\alpha}\end{array}\right)-\left(\begin{array}{c}\beta \\ \alpha\end{array}\right)\right],
\end{aligned}
\end{equation*}

where $\left(\begin{array}{l}\hat{\beta}^{*} \\ \hat{\alpha}^{*}\end{array}\right)$ lies on the line segment between $\left(\begin{array}{l}\beta \\ \alpha\end{array}\right)$ and $\left(\begin{array}{l}\hat{\beta} \\ \hat{\alpha}\end{array}\right).$

\end{Problem}

From the first order Taylor Expansion, we can obtain that
\begin{equation*}
  \begin{aligned}
    n^{1/2}\left[\left(\begin{array}{l}\hat{\beta} \\ \hat{\alpha}\end{array}\right)-\left(\begin{array}{c}\beta \\ \alpha\end{array}\right)\right] 
&=
-n
\left.\left(\begin{array}{ll}\dfrac{\partial S_{\beta}(\beta, \alpha)}{\partial \beta} & \dfrac{\partial S_{\beta}(\beta, \alpha)}{\partial \alpha} \\ \dfrac{\partial S_{\alpha}(\beta, \alpha)}{\partial \beta} & \dfrac{\partial S_{\alpha}(\beta, \alpha)}{\partial \alpha}\end{array}\right)^{-1}\right|_{\begin{pmatrix}\left.\hat{\beta}^{*}\right. \\ \hat{\alpha}^{*}\end{pmatrix}}
n^{-1/2}
\left(\begin{array}{l}S_{\beta}(\beta, \alpha) \\ S_{\alpha}(\beta, \alpha)\end{array}\right)\\
& = -n
% +\times 
V^{* -1}\,
n^{-1/2}
\left(\begin{array}{l}S_{\beta}(\beta, \alpha) \\ S_{\alpha}(\beta, \alpha)\end{array}\right).
  \end{aligned}
\end{equation*}

We have

\begin{equation*}
  \begin{aligned}
    V^{*}\xrightarrow{p} V 
    &= \E\left[\left(\begin{array}{ll}\dfrac{\partial S_{\beta}(\beta, \alpha)}{\partial \beta} & \dfrac{\partial S_{\beta}(\beta, \alpha)}{\partial \alpha} \\ \dfrac{\partial S_{\alpha}(\beta, \alpha)}{\partial \beta} & \dfrac{\partial S_{\alpha}(\beta, \alpha)}{\partial \alpha}\end{array}\right)\right]\\
    &=\left(\begin{array}{ll}\left(\dfrac{\partial \mu_{i}}{\partial \beta}\right)^{T}[\operatorname{Var}(Y_i)]^{-1}\left(\dfrac{\partial \mu_{i}}{\partial \beta}\right) 
      & \left(\dfrac{\partial \mu_{i}}{\partial \beta}\right)^{T}[\operatorname{Var}(Y_i)]^{-1}\left(\dfrac{\partial \mu_{i}}{\partial \alpha}\right) \\ 
      \left(\dfrac{\partial \eta_{i}}{\partial \alpha}\right)^{T} H_{i}^{-1}\left(\dfrac{\partial \eta_{i}}{\partial \beta}\right) 
      & \left(\dfrac{\partial \eta_{i}}{\partial \alpha}\right)^{T} H_{i}^{-1}\left(\dfrac{\partial \eta_{i}}{\partial \alpha}\right)\end{array}\right)\\
      &=\E[C_i^TB_i^{-1}D_i],
  \end{aligned}
\end{equation*}
which is estimated by \(\left(\frac{1}{n} \sum_{i=1}^{n} C_{i}^{T} B_{i}^{-1} D_{i}\right)\).

---


\begin{equation*}
  \begin{aligned}
  S_{\beta}(\beta, \alpha)
  &=\sum_{i=1}^{n}\left(\dfrac{\partial \mu_{i}}{\partial \beta}\right)^{T}\left[\operatorname{Var}\left(Y_{i}\right)\right]^{-1}\left(Y_{i}-\mu_{i}\right)\\
  &=\sum_{i=1}^{n}U_i.
\end{aligned}
\end{equation*}

\begin{equation*}
  \begin{aligned}
S_{\alpha}(\beta, \alpha)
&=\sum_{i=1}^{n}\left(\dfrac{\partial \eta_{i}}{\partial \alpha}\right)^{T} H_{i}^{-1}\left(\omega_{i}-\eta_{i}\right)\\
&=\sum_{i=1}^{n}Z_i.
\end{aligned}
\end{equation*}

By CLT,
\begin{equation*}
  \begin{aligned}
    n^{-1/2}S_{\beta}(\beta, \alpha)=\sqrt{n}\frac{1}{n}\sum_{i=1}^{n}U_i = \sqrt{n}(\bar{U}_i - \E[U_i]) \xrightarrow{d} N(0,\sigma_{U}^2),
  \end{aligned}
\end{equation*}
where
\begin{equation*}
  \begin{aligned}
    \sigma_{U}^2 
    &= \operatorname{Var}[U_i] \\
    &= \operatorname{Var}[\left(\dfrac{\partial \mu_{i}}{\partial \beta}\right)^{T}\left[\operatorname{Var}\left(Y_{i}\right)\right]^{-1}\left(Y_{i}-\mu_{i}\right)]\\
    &=\left(\dfrac{\partial \mu_{i}}{\partial \beta}\right)^{T}\left[\operatorname{Var}\left(Y_{i}\right)\right]^{-2}\left(\dfrac{\partial \mu_{i}}{\partial \beta}\right)\operatorname{Var}(Y_i)\\
    &=\left(\dfrac{\partial \mu_{i}}{\partial \beta}\right)^{T}\left[\operatorname{Var}\left(Y_{i}\right)\right]^{-1}\left(\dfrac{\partial \mu_{i}}{\partial \beta}\right)
  \end{aligned}
\end{equation*}


Similarly,



\begin{equation*}
  \begin{aligned}
    n^{-1/2}S_{\alpha}(\beta, \alpha)=\sqrt{n}\frac{1}{n}\sum_{i=1}^{n}Z_i = \sqrt{n}(\bar{Z}_i - \E[Z_i]) \xrightarrow{d} N(0,\sigma_{Z}^2),
  \end{aligned}
\end{equation*}
where
\begin{equation*}
  \begin{aligned}
    \sigma_{Z}^2 
    &= \operatorname{Var}[Z_i] \\
    &= \operatorname{Var}[\left(\dfrac{\partial \eta_{i}}{\partial \alpha}\right)^{T} H_{i}^{-1}\left(\omega_{i}-\eta_{i}\right)]\\
    &=\left(\dfrac{\partial \eta_{i}}{\partial \alpha}\right)^{T} H_{i}^{-2}\operatorname{Var}[\omega_i]\left(\dfrac{\partial \eta_{i}}{\partial \alpha}\right)\\
    &=\left(\dfrac{\partial \eta_{i}}{\partial \alpha}\right)^{T} H_{i}^{-1}\left(\dfrac{\partial \eta_{i}}{\partial \alpha}\right).
  \end{aligned}
\end{equation*}

Thus,

\begin{equation*}
  \begin{aligned}
n^{-1/2}
\left(\begin{array}{l}S_{\beta}(\beta, \alpha) \\ S_{\alpha}(\beta, \alpha)\end{array}\right)\xrightarrow{d} N(0,\Sigma_{S}),
  \end{aligned}
\end{equation*}
where
\begin{equation*}
  \begin{aligned}
    \Sigma_{S} = \begin{pmatrix}
      \sigma_{U}^2 & \sigma_{UZ}^2\\
      \sigma_{UZ}^2 & \sigma_{Z}^2
    \end{pmatrix},
  \end{aligned}
\end{equation*}
with

\begin{equation*}
  \begin{aligned}
    \sigma_{UZ}^2 &= \Cov(U_i,Z_i) = \E[U_i Z_i] \\
    % &= \E[\{\left(\dfrac{\partial \mu_{i}}{\partial \beta}\right)^{T}\left[\operatorname{Var}\left(Y_{i}\right)\right]^{-1}\left(Y_{i}-\mu_{i}\right)\} \{\left(\dfrac{\partial \eta_{i}}{\partial \alpha}\right)^{T} H_{i}^{-1}\left(\omega_{i}-\eta_{i}\right)\}] 
    \\
    % &= \Cov[\{\left(\dfrac{\partial \mu_{i}}{\partial \beta}\right)^{T}\left[\operatorname{Var}\left(Y_{i}\right)\right]^{-1}\left(Y_{i}-\mu_{i}\right)\}, \{\left(\dfrac{\partial \eta_{i}}{\partial \alpha}\right)^{T} H_{i}^{-1}\left(\omega_{i}-\eta_{i}\right)\}]\\
  &= \E\left[\left(\dfrac{\partial \mu_{i}}{\partial \beta}\right)^{T}\left[\operatorname{Var}\left(Y_{i}\right)\right]^{-1}\left(Y_{i}-\mu_{i}\right) \cdot \left(\dfrac{\partial \eta_{i}}{\partial \alpha}\right)^{T} H_{i}^{-1}\left(\omega_{i}-\eta_{i}\right)\right].
  \end{aligned}
\end{equation*}

Since

\begin{equation*}
  \begin{aligned}
    &\E(C_{i}^{T} B_{i}^{-1} V_{0 i} B_{i}^{-1} C_{i}) \\
    &= \E\left[C_{i}^{T} B_{i}^{-1}
    \begin{pmatrix}
      (y_i-\mu_i)^2&(y_i-\mu_i)(\omega_i-\eta_i)\\
      (y_i-\mu_i)(\omega_i-\eta_i)&(\omega_i-\eta_i)^2\\
    \end{pmatrix}
    B_{i}^{-1} C_{i}
    \right],
  \end{aligned}
\end{equation*}
where

\begin{equation*}
  \begin{aligned}
    C_{i}^{T} B_{i}^{-1} = \begin{pmatrix}
      \left(\dfrac{\partial \mu_{i}}{\partial \beta}\right)^{T}[\operatorname{Var}(Y_i)]^{-1}
      & 0\\ 
      0
      & \left(\dfrac{\partial \eta_{i}}{\partial \alpha}\right)^{T} H_{i}^{-1}
    \end{pmatrix},
  \end{aligned}
\end{equation*}

we can estimate \(\Sigma_S\) with \(\left(\frac{1}{n} \sum_{i=1}^{n} C_{i}^{T} B_{i}^{-1} V_{0 i} B_{i}^{-1} C_{i}\right)\).

By Slutsky Theorem,

\begin{equation*}
  \begin{aligned}
    n^{1/2}\left[\left(\begin{array}{l}\hat{\beta} \\ \hat{\alpha}\end{array}\right)-\left(\begin{array}{c}\beta \\ \alpha\end{array}\right)\right] \xrightarrow{d} N(0,\Sigma),
  \end{aligned}
\end{equation*}
where \(\Sigma = V^{-T}\Sigma_S V^{-1}\).

Therefore, $\Sigma$ can be estimated by 
\[\left(\frac{1}{n} \sum_{i=1}^{n} C_{i}^{T} B_{i}^{-1} D_{i}\right)^{-1}\left(\frac{1}{n} \sum_{i=1}^{n} C_{i}^{T} B_{i}^{-1} V_{0 i} B_{i}^{-1} C_{i}\right)\left(\frac{1}{n} \sum_{i=1}^{n} D_{i}^{T} B_{i}^{-1} C_{i}\right)^{-1},\]

% ---


% Thanks to D11221003


% \begin{equation*}
%   \begin{aligned}
%   \end{aligned}
% \end{equation*}

\end{document}