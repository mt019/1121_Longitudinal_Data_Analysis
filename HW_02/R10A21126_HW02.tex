\documentclass[UTF8,a4paper,10pt]{article}

\input{preamble.tex}

% \begin{equation*}
%   \begin{aligned}
%   \end{aligned}
% \end{equation*}

% \begin{mybox}{}
% \end{mybox}


% \begin{Problem}[]{}
% \end{Problem}

% \begin{solution}\,
% \end{solution}
  

% \begin{enumerate}[label=(\alph*)]
% \end{enumerate} 

% \setcounter{section}{3} 
% \setcounter{theorem}{3}

% \begin{theorem}\label{thm:3.4}
%   If $E = \bigcup_{k} E_k$ is a countable union of sets, then $|E|_e \leq \sum_{k} |E_k|_e$.
%   \end{theorem}

%  \footcite[][42]{Wheeden_Zygmund_2015}

\begin{document}


\begin{mybox}{}
  Kernel estimator: 
  \begin{equation*}
    \begin{aligned}
      \omega(u,v)\triangleq \frac{1}{h}k(\frac{u-v}{h})
    \end{aligned}
  \end{equation*}
  with \(h>0\).
  


\end{mybox}


% Q1


  \begin{Problem}[]{}
    Find the form of GLM for the following distributions, and show the resonable link function:
    \begin{enumerate}
      \item Binomial distribution
      \item Poisson distribution
      \item Gamma distribution (location shift)
      \item Normal distributions
      \item Beta (重新參數化)
      \item Inverse Gaussian
    \end{enumerate}
    
  \end{Problem}

  
  \begin{Problem}[]{}
  Paper Summarization
    
  \end{Problem}

  The part 1 of this page talks about:

- **Sliced inverse regression (SIR)**: A novel data-analytic tool for reducing the dimension of the input variable x without fitting any parametric or nonparametric model¹[1]. It explores the inverse view of regression, where x is regressed against y, and uses a simple step function to estimate the inverse regression curve²[2].
- **Effective dimension reduction (e.d.r.) space**: The linear space generated by the unknown row vectors {3k (k = 1, ..., K) in the model y = f({3lx, ..., {3Kx, e), where f is an arbitrary unknown function. The goal is to estimate this space, which captures all the information about y from x.
- **Inverse regression curve**: The curve E(x I y) that connects the conditional mean of x given y as y varies. Under certain conditions, this curve falls into the e.d.r. space. A principal component analysis on the covariance matrix of the estimated inverse regression curve can locate its main orientation, yielding the estimates for e.d.r. directions³[3].
- **Sampling properties of SIR**: The output of SIR provides root n consistent estimates for the e.d.r. directions under a design condition on the distribution of x⁴[4]. The eigenvalues of the covariance matrix can be used to assess the number of components in the model and the effectiveness of SIR.
- **Simulation results**: SIR is demonstrated to be effective in reducing the dimension of x from 10 to 2 for a data set with 400 observations. The spin-plot of y against the projected variables obtained by SIR mimics the spin-plot of y against the true directions very well⁵[5]. A chi-squared statistic is proposed to test whether a direction found by SIR is spurious⁶[6].

\begin{Problem}[]{}
  Describe
  \begin{enumerate}
    \item how to conduct  maximum likelihood  for the random effects model
    \item how to conduct  MCMC
  \end{enumerate}
    
  \end{Problem}
  
  Maximum Likelihood for the Random Effects Model:

Maximum Likelihood Estimation (MLE) is a method used to estimate the parameters of a statistical model, and it can be applied to random effects models. In a random effects model, you typically have two levels of variation: random effects (which are specific to each group or cluster) and fixed effects (which are common across all groups). Here's how you can conduct MLE for a random effects model:

a. Model Specification: Start by specifying the random effects model, which includes the fixed effects and random effects components. For example, in a linear mixed-effects model, you may have:

\[Y_ij = β_0 + β_1*X_ij + u_i + ε_ij\]


\(Y_ij\) is the observed response for the jth observation in the ith group.

\(β_0\) and \(β_1 \)are fixed effect coefficients.

\(u_i\) represents the random effect specific to group i.

\(ε_ij\) is the residual error.


b. Likelihood Function: Construct the likelihood function, which describes how likely the observed data is given the model parameters. In a random effects model, this includes both fixed and random effect parameters. The likelihood function is often assumed to follow a specific distribution, such as the normal distribution for the random effects and residuals.

c. Maximize the Likelihood: The goal is to find parameter estimates (β's and the variance components for the random effects and residuals) that maximize the likelihood function. This involves iterative optimization techniques, such as the EM (Expectation-Maximization) algorithm or specialized software like the lme4 package in R for mixed-effects models.

d. Estimation of Random Effects: Once the fixed effects parameters are estimated, you can obtain estimates of the random effects for each group. The random effects capture the group-specific deviations from the fixed effects.

e. Inference: After obtaining parameter estimates, you can perform statistical inference, such as hypothesis tests or confidence intervals, to assess the significance of fixed effects and variances of random effects.

Markov Chain Monte Carlo (MCMC):

Markov Chain Monte Carlo is a statistical method used to estimate the posterior distribution of model parameters in Bayesian statistics. MCMC is particularly useful when you cannot directly compute the posterior distribution analytically, which is often the case in complex models. Here's how you can conduct MCMC:

a. Model Specification: Start by specifying the Bayesian model, which includes the likelihood function (describing data likelihood) and the prior distribution (describing your beliefs about the parameters before seeing data).

b. Initialization: Choose initial values for all model parameters.

c. MCMC Algorithm: Run the MCMC algorithm, such as the Metropolis-Hastings or Gibbs sampler, to sample from the joint posterior distribution of the parameters. In each iteration, you propose new parameter values based on the current values and accept or reject them according to a specific acceptance rule (e.g., Metropolis-Hastings acceptance ratio).

d. Burn-in Period: Discard a certain number of initial samples (burn-in) to allow the Markov chain to converge to the stationary distribution.

e. Sampling: After the burn-in period, collect samples from the Markov chain at regular intervals. These samples represent draws from the posterior distribution of the model parameters.

f. Inference: Use the obtained samples to make inferences about the model parameters. You can compute posterior means, medians, credible intervals, and perform various analyses based on the posterior samples.

MCMC is a powerful technique for estimating complex models, especially in Bayesian statistics. It allows you to quantify uncertainty about model parameters and make probabilistic statements about your inferences.


\begin{Problem}[]{}
  Under the regularity conditions,
  \begin{equation*}
    \begin{aligned}
      n^{-\frac{1}{2}}\big[\begin{pmatrix} \hat{\beta}\\ \hat{\alpha}\end{pmatrix} - \begin{pmatrix} \beta\\ \alpha\end{pmatrix}  \big]\overset{d}{\to } N(0,\Sigma),
    \end{aligned}
  \end{equation*} 
  where \(Sigma\) can be estimated By
  \begin{equation*}
    \begin{aligned}
    \end{aligned}
  \end{equation*} 
    
  \end{Problem}
  

\end{document}