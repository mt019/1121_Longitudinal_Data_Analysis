\documentclass[UTF8,a4paper,10pt]{article}

\input{preamble.tex}

% \begin{equation*}
%   \begin{aligned}
%   \end{aligned}
% \end{equation*}

% \begin{mybox}{}
% \end{mybox}


% \begin{Problem}[]{}
% \end{Problem}

% \begin{solution}\,
% \end{solution}
  

% \begin{enumerate}[label=(\alph*)]
% \end{enumerate} 

% \setcounter{section}{3} 
% \setcounter{theorem}{3}

% \begin{theorem}\label{thm:3.4}
%   If $E = \bigcup_{k} E_k$ is a countable union of sets, then $|E|_e \leq \sum_{k} |E_k|_e$.
%   \end{theorem}

%  \footcite[][42]{Wheeden_Zygmund_2015}

\begin{document}


\begin{mybox}{}


\end{mybox}



% Q1


\begin{Problem}[]{Inversion Theorems}
  Show the one-to-one relationship between the probability distribution function and the characteristic function. Express the probability distribution function as a function of the characteristic function.

\end{Problem}


\begin{Problem}[]{}
  Explain why the moment generation function does not exist when a certain moment is absent. Provide an explanation for the absence of a moment in the context of the logistic distribution.
\end{Problem}

\begin{Problem}[]{}
  Logistic distribution 有對稱性。

  説明：其他model沒有對稱性
\end{Problem}

As a set of independent binary regressions, to arrive at the multinomial logit model, one can imagine, for $K$ possible outcomes, running $K-1$ independent binary logistic regression models, in which one outcome is chosen as a "pivot" and then the other $K-1$ outcomes are separately regressed against the pivot outcome. If outcome $K$ (the last outcome) is chosen as the pivot, the $K-1$ regression equations are:

\begin{equation}
\ln\left(\frac{\Pr(Y_i=k)}{\Pr(Y_i=K)}\right) = \boldsymbol{\beta}_k \cdot \mathbf{X}_i, \quad k < K.
\end{equation}

This formulation is also known as the Additive Log Ratio transform, commonly used in compositional data analysis. In other applications, it's referred to as "relative risk" [7].

If we exponentiate both sides and solve for the probabilities, we get:

\begin{equation}
\Pr(Y_i=k) = \Pr(Y_i=K) \cdot e^{\boldsymbol{\beta}_k \cdot \mathbf{X}_i}, \quad k < K.
\end{equation}

Using the fact that all $K$ of the probabilities must sum to one, we find:

\begin{equation}
\Pr(Y_i=K) = 1 - \sum_{j=1}^{K-1} \Pr(Y_i=j) = 1 - \sum_{j=1}^{K-1} \Pr(Y_i=K) \cdot e^{\boldsymbol{\beta}_j \cdot \mathbf{X}_i} \Rightarrow \Pr(Y_i=K) = \frac{1}{1 + \sum_{j=1}^{K-1} e^{\boldsymbol{\beta}_j \cdot \mathbf{X}_i}}.
\end{equation}

We can use this to find the other probabilities:

\begin{equation}
\Pr(Y_i=k) = \frac{e^{\boldsymbol{\beta}_k \cdot \mathbf{X}_i}}{1 + \sum_{j=1}^{K-1} e^{\boldsymbol{\beta}_j \cdot \mathbf{X}_i}}, \quad k < K.
\end{equation}

\begin{equation}
  \Pr(Y_i=l) = \frac{\Id(l = K)+\Id(l\neq K)e^{\boldsymbol{\beta}_l \cdot \mathbf{X}_i}}{1 + \sum_{j\neq K} e^{\boldsymbol{\beta}_j \cdot \mathbf{X}_i}}, l = 1,2,\cdots,K.
  \end{equation}

The fact that we run multiple regressions reveals why the model relies on the assumption of independence of irrelevant alternatives described above.



% \begin{Problem}[]{}
%   Logistic distribution

%   \(Y\in\{0,1\}\)

    
% \begin{equation*}
%   \begin{aligned}
%    F_0(v) = \dfrac{\exp(-v)}{1+\exp(-v)}
%   \end{aligned}
% \end{equation*}

% \begin{equation*}
%   \begin{aligned}
%     P(Y=1|X=x) = \dfrac{\exp(\beta_0^T z)}{1+\exp(\beta_0^T z)}\overset{\bigtriangleup }{=}F_0(\beta_0^T z)
%   \end{aligned}
% \end{equation*}

% The standard Logistic distribution is symmetric about the origin but not about its mean. 

% The probability density function (PDF) for the standard Logistic distribution is given by:

% \[ f(v) = \dfrac{e^{-v}}{(1+e^{-v})^2} \]

% To demonstrate its symmetry, let's evaluate the PDF at \(v\) and \(-v\):

% \[ f(v) = \dfrac{e^{-v}}{(1+e^{-v})^2} \]

% \[ f(-v) = \dfrac{e^{v}}{(1+e^{v})^2} \]

% % As you can see, \(f(v)\) and \(f(-v)\) are not equal, indicating that the standard Logistic distribution is not symmetric about its mean. It is symmetric about the origin (0), where \(v = 0\), but not around its mean, which is also 0 for the standard Logistic distribution.


% \end{Problem}


\pagebreak

\begin{Problem}[]{}
  (b.1) Log-linear model: 
  
  \[P(Y=y)=C\left(\theta_1, \theta_2\right) \exp \left(Y^T \theta_1+W^T \theta_2\right),\] 
  
  where 

  \begin{itemize}
    \item \(W=\left(Y_1 Y_2, Y_1 Y_3, \cdots, Y_{m-1} Y_m, \cdots, Y_1 Y_2 \cdots Y_m\right)^T, \)
    \item \(\theta_1=\left(\theta_1^{(1)}, \cdots, \theta_m^{(1)}\right)\)
    \item \(\theta_2=\left(\theta_{12}^{(2)}, \cdots, \theta_{m-1 m}^{(2)}, \cdots, \theta_{12 \cdots m}^{(m)}\right)\)
    \item \(C(\theta_1, \theta_2)\) is a function of \(\theta_1\) and \(\theta_2\) that normalizes the p.d.f. to integrate to one.

  \end{itemize}


Transformation: 

\[\left(\theta_1, \theta_2\right) \rightarrow\left(\mu, \theta_2\right), \mu=\left(\mu_1, \ldots, \mu_m\right) \triangleq \mu\left(\theta_1, \theta_2\right).\]

Model assumption: 

\[\operatorname{logit}\left(\mu_j\right)=X_j^T \beta.\]

The score equation for $\beta$ under this parameterization takes the GEE form:


\[\left(\frac{\partial \mu}{\partial \beta}\right)^T[V(Y)]^{-1}(Y-\mu)=0,\] 

where  

\[\frac{\partial \mu}{\partial \beta}=\left(\frac{\partial \mu_1}{\partial \beta}, \ldots, \frac{\partial \mu_m}{\partial \beta}\right)^T.\]


\textbf{Remark: }

The conditional odds ratios are not easily interpreted when the association among responses is itself a focus of the study.

\textbf{Properties: }

1. From 

\[M_Y(t)=E\left[e^{t^T Y}\right]=\sum_y C\left(\theta_1, \theta_2\right) \exp \left(y^T\left(t+\theta_1\right)+w^T \theta_2\right)=\frac{C\left(\theta_1, \theta_2\right)}{C\left(\theta_1+t, \theta_2\right)},\] 

one has

\begin{itemize}
  \item \[\mu=\left.\frac{\partial M_Y(t)}{\partial t}\right|_{t=0}=-\dfrac{\frac{\partial}{\partial \theta_1} C\left(\theta_1, \theta_2\right)}{C\left(\theta_1, \theta_2\right)},\]
  \item \[E\left[Y Y^T\right]=\left.\dfrac{\partial^2 M_Y(t)}{\partial t \partial t^T}\right|_{t=0}=-\frac{\frac{\partial^2}{\partial \theta_1 \partial \theta_1^T} C\left(\theta_1, \theta_2\right)}{C\left(\theta_1, \theta_2\right)}+2 \mu \mu^T,\]
  \item \[V(Y)=-\dfrac{\frac{\partial^2}{\partial \theta_1 \partial \theta_1^T} C\left(\theta_1, \theta_2\right)}{C\left(\theta_1, \theta_2\right)}+\mu \mu^T.\]
\end{itemize}

2. Let 

\[l(\theta_1,\theta_2) = \ln P(Y=y) = \ln \{C(\theta_1,\theta_2)+(Y^T\theta_1 +W^T \theta_2)\}.\]

We can derive that

\[\dfrac{\partial l\left(\theta_1, \theta_2\right)}{\partial \theta_1}=\dfrac{\frac{\partial}{\partial \theta_1} C\left(\theta_1, \theta_2\right)}{C\left(\theta_1, \theta_2\right)}+Y=(Y-\mu),\]

 and hence,
 
$$
\frac{\partial l\left(\theta_1, \theta_2\right)}{\partial \beta}=\left(\frac{\partial \mu}{\partial \beta}\right)^T \frac{\partial \theta_1}{\partial \mu}\left(\frac{\partial l\left(\theta_1, \theta_2\right)}{\partial \theta_1}\right)=\left(\frac{\partial \mu}{\partial \beta}\right)^T\left(\frac{\partial \mu}{\partial \theta_1}\right)^{-1}(Y-\mu)=\left(\frac{\partial \mu}{\partial \beta}\right)^T[V(Y)]^{-1}(Y-\mu) .
$$
\end{Problem}

\begin{Problem}[]{Show  \textbf{orthonormal}}
Let $P_{[1]}(Y=y)=\prod_{j=1}^m \mu_j^{y_j}\left(1-\mu_j\right)^{1-y_j}, g(y)=P(Y=y) / P_{[1]}(Y=y)$, and $V$ be a vector space of real-valued functions $f$ on $Y_1\left(2^m\right.$ possible values of $\left.y\right)$. 

Here, $V$ is regarded as an inner-product space with

\[ <f_1, f_2>\triangleq E_{P_{[1]}}\left[f_1 f_2\right]=\sum_{y \in Y_1} f_1(y) f_2(y) P_{[1]}(y).\]

It follows easily that the set of functions $S=\left\{1, r_1, \ldots, r_m ; r_1 r_2, \ldots, r_{m-1} r_m ; \ldots, r_1 r_2 \cdots r_m\right\}$ on $Y_1$ is \textbf{orthonormal} and, thus, is a basis in $V$.
 Since $g(y)$ is a function on $Y_1$, there exists a unique representation as a linear combination of functions in $S$, namely,
$$
\begin{aligned}
& g(y)=\sum_{f \in S}<g, f>f . \\
& \because<g, f>=\sum_{y \in Y_1} g(y) f(y) P_{[1]}(y)=\sum_{y \in Y_1} f(y) P(Y=y)=E_P[f] \,\forall f, \text { and } \\
& E_P[1]=1, E_P\left[r_j\right]=0, E_P\left[r_j r_k\right]=\rho_{j k}, \cdots, \text { and } E_P\left[r_1 \cdots r_m\right]=\rho_{12 \cdots m} . \\
& \therefore g(y)=\left(1+\sum_{j<k} \rho_{j k} r_j r_k+\sum_{j<k<l} \rho_{j k l} r_j r_k r_l+\cdots+\rho_{12 \cdots m} r_1 r_2 \cdots r_m\right) .
\end{aligned}
$$
\end{Problem}

\pagebreak

Paper Summary


Here is a summary of this paper:

\begin{itemize}
  \item \textbf{Research Problem}: The paper addresses the challenge of analyzing longitudinal data on disease markers in AIDS cohort studies, which involves censoring, truncation, and correlation issues.
  \item \textbf{Methodology}: The paper proposes a likelihood-based approach that models the joint distribution of the disease markers, the time of infection, and the time to AIDS using parametric models. The paper also handles the censoring and truncation problems using standard survival analysis techniques and provides a method for predicting the time to AIDS given a series of disease marker measurements.
  \item \textbf{Data and Application}: The paper applies the proposed method to the Toronto AIDS cohort study, which consists of 159 homosexual males who were infected with HIV or seroconverted during follow-up. The paper analyzes the longitudinal data on T4 counts and T4/T8 ratio as disease markers and compares three models with different assumptions about the infection time.
  \item \textbf{Results and Conclusion}: The paper finds that there is a significant association between the rate of decline of T4 counts or T4/T8 ratio and the time to AIDS, and that the scaled incubation time may be a more natural scale for viewing the progression of HIV infection. The paper also shows that the disease marker information improves the prediction of the time to AIDS. The paper concludes that the proposed method is useful for understanding the natural history of AIDS and developing treatment strategies.
\end{itemize}


\begin{equation*}
  \begin{aligned}
  \end{aligned}
\end{equation*}

\end{document}

