\documentclass[UTF8,a4paper,10pt]{article}

\input{preamble.tex}

% \begin{equation*}
%   \begin{aligned}
%   \end{aligned}
% \end{equation*}

% \begin{mybox}{}
% \end{mybox}


% \begin{Problem}[]{}
% \end{Problem}

% \begin{solution}\,
% \end{solution}
  

% \begin{enumerate}[label=(\alph*)]
% \end{enumerate} 

% \setcounter{section}{3} 
% \setcounter{theorem}{3}

% \begin{theorem}\label{thm:3.4}
%   If $E = \bigcup_{k} E_k$ is a countable union of sets, then $|E|_e \leq \sum_{k} |E_k|_e$.
%   \end{theorem}

%  \footcite[][42]{Wheeden_Zygmund_2015}

\begin{document}


\begin{mybox}{}


\end{mybox}



% Q1


\begin{Problem}[]{Inversion Theorems}
  Show the one-to-one relationship between the probability distribution function and the characteristic function. Express the probability distribution function as a function of the characteristic function.

\end{Problem}


\begin{Problem}[]{}
  Explain why the moment generation function does not exist when a certain moment is absent. Provide an explanation for the absence of a moment in the context of the logistic distribution.
\end{Problem}

\begin{Problem}[]{}
  Logistic distribution 有對稱性。

  説明：其他model沒有對稱性
\end{Problem}

% \begin{Problem}[]{}
%   Logistic distribution

%   \(Y\in\{0,1\}\)

    
% \begin{equation*}
%   \begin{aligned}
%    F_0(v) = \dfrac{\exp(-v)}{1+\exp(-v)}
%   \end{aligned}
% \end{equation*}

% \begin{equation*}
%   \begin{aligned}
%     P(Y=1|X=x) = \dfrac{\exp(\beta_0^T z)}{1+\exp(\beta_0^T z)}\overset{\bigtriangleup }{=}F_0(\beta_0^T z)
%   \end{aligned}
% \end{equation*}

% The standard Logistic distribution is symmetric about the origin but not about its mean. 

% The probability density function (PDF) for the standard Logistic distribution is given by:

% \[ f(v) = \dfrac{e^{-v}}{(1+e^{-v})^2} \]

% To demonstrate its symmetry, let's evaluate the PDF at \(v\) and \(-v\):

% \[ f(v) = \dfrac{e^{-v}}{(1+e^{-v})^2} \]

% \[ f(-v) = \dfrac{e^{v}}{(1+e^{v})^2} \]

% % As you can see, \(f(v)\) and \(f(-v)\) are not equal, indicating that the standard Logistic distribution is not symmetric about its mean. It is symmetric about the origin (0), where \(v = 0\), but not around its mean, which is also 0 for the standard Logistic distribution.


% \end{Problem}


\pagebreak

\begin{Problem}[]{}
  (b.1) Log-linear model: 
  
  \[P(Y=y)=C\left(\theta_1, \theta_2\right) \exp \left(Y^T \theta_1+W^T \theta_2\right),\] 
  
  where 

  \begin{itemize}
    \item \(W=\left(Y_1 Y_2, Y_1 Y_3, \cdots, Y_{m-1} Y_m, \cdots, Y_1 Y_2 \cdots Y_m\right)^T, \)
    \item \(\theta_1=\left(\theta_1^{(1)}, \cdots, \theta_m^{(1)}\right)\)
    \item \(\theta_2=\left(\theta_{12}^{(2)}, \cdots, \theta_{m-1 m}^{(2)}, \cdots, \theta_{12 \cdots m}^{(m)}\right)\)
    \item \(C(\theta_1, \theta_2)\) is a function of \(\theta_1\) and \(\theta_2\) that normalizes the p.d.f. to integrate to one.

  \end{itemize}


Transformation: 

\[\left(\theta_1, \theta_2\right) \rightarrow\left(\mu, \theta_2\right), \mu=\left(\mu_1, \ldots, \mu_m\right) \triangleq \mu\left(\theta_1, \theta_2\right).\]

Model assumption: 

\[\operatorname{logit}\left(\mu_j\right)=X_j^T \beta.\]

The score equation for $\beta$ under this parameterization takes the GEE form:


\[\left(\frac{\partial \mu}{\partial \beta}\right)^T[V(Y)]^{-1}(Y-\mu)=0,\] 

where  

\[\frac{\partial \mu}{\partial \beta}=\left(\frac{\partial \mu_1}{\partial \beta}, \ldots, \frac{\partial \mu_m}{\partial \beta}\right)^T.\]


\textbf{Remark: }

The conditional odds ratios are not easily interpreted when the association among responses is itself a focus of the study.

\textbf{Properties: }

1. From 

\[M_Y(t)=E\left[e^{t^T Y}\right]=\sum_y C\left(\theta_1, \theta_2\right) \exp \left(y^T\left(t+\theta_1\right)+w^T \theta_2\right)=\frac{C\left(\theta_1, \theta_2\right)}{C\left(\theta_1+t, \theta_2\right)},\] 

one has

\begin{itemize}
  \item \[\mu=\left.\frac{\partial M_Y(t)}{\partial t}\right|_{t=0}=-\dfrac{\frac{\partial}{\partial \theta_1} C\left(\theta_1, \theta_2\right)}{C\left(\theta_1, \theta_2\right)},\]
  \item \[E\left[Y Y^T\right]=\left.\dfrac{\partial^2 M_Y(t)}{\partial t \partial t^T}\right|_{t=0}=-\frac{\frac{\partial^2}{\partial \theta_1 \partial \theta_1^T} C\left(\theta_1, \theta_2\right)}{C\left(\theta_1, \theta_2\right)}+2 \mu \mu^T,\]
  \item \[V(Y)=-\dfrac{\frac{\partial^2}{\partial \theta_1 \partial \theta_1^T} C\left(\theta_1, \theta_2\right)}{C\left(\theta_1, \theta_2\right)}+\mu \mu^T.\]
\end{itemize}

2. Let 

\[l(\theta_1,\theta_2) = \ln P(Y=y) = \ln \{C(\theta_1,\theta_2)+(Y^T\theta_1 +W^T \theta_2)\}.\]

We can derive that

\[\dfrac{\partial l\left(\theta_1, \theta_2\right)}{\partial \theta_1}=\dfrac{\frac{\partial}{\partial \theta_1} C\left(\theta_1, \theta_2\right)}{C\left(\theta_1, \theta_2\right)}+Y=(Y-\mu),\]

 and hence,
 
$$
\frac{\partial l\left(\theta_1, \theta_2\right)}{\partial \beta}=\left(\frac{\partial \mu}{\partial \beta}\right)^T \frac{\partial \theta_1}{\partial \mu}\left(\frac{\partial l\left(\theta_1, \theta_2\right)}{\partial \theta_1}\right)=\left(\frac{\partial \mu}{\partial \beta}\right)^T\left(\frac{\partial \mu}{\partial \theta_1}\right)^{-1}(Y-\mu)=\left(\frac{\partial \mu}{\partial \beta}\right)^T[V(Y)]^{-1}(Y-\mu) .
$$
\end{Problem}

\begin{Problem}[]{Show  \textbf{orthonormal}}
Let $P_{[1]}(Y=y)=\prod_{j=1}^m \mu_j^{y_j}\left(1-\mu_j\right)^{1-y_j}, g(y)=P(Y=y) / P_{[1]}(Y=y)$, and $V$ be a vector space of real-valued functions $f$ on $Y_1\left(2^m\right.$ possible values of $\left.y\right)$. 

Here, $V$ is regarded as an inner-product space with

\[ <f_1, f_2>\triangleq E_{P_{[1]}}\left[f_1 f_2\right]=\sum_{y \in Y_1} f_1(y) f_2(y) P_{[1]}(y).\]

It follows easily that the set of functions $S=\left\{1, r_1, \ldots, r_m ; r_1 r_2, \ldots, r_{m-1} r_m ; \ldots, r_1 r_2 \cdots r_m\right\}$ on $Y_1$ is \textbf{orthonormal} and, thus, is a basis in $V_{\times}$Since $g(y)$ is a function on $Y_1$, there exists a unique representation as a linear combination of functions in $S$, namely,
$$
\begin{aligned}
& g(y)=\sum_{f \in S}<g, f>f . \\
& \because<g, f>=\sum_{y \in Y_1} g(y) f(y) P_{[1]}(y)=\sum_{y \in Y_1} f(y) P(Y=y)=E_P[f] \,\forall f, \text { and } \\
& E_P[1]=1, E_P\left[r_j\right]=0, E_P\left[r_j r_k\right]=\rho_{j k}, \cdots, \text { and } E_P\left[r_1 \cdots r_m\right]=\rho_{12 \cdots m} . \\
& \therefore g(y)=\left(1+\sum_{j<k} \rho_{j k} r_j r_k+\sum_{j<k<l} \rho_{j k l} r_j r_k r_l+\cdots+\rho_{12 \cdots m} r_1 r_2 \cdots r_m\right) .
\end{aligned}
$$
\end{Problem}

\begin{equation*}
  \begin{aligned}
  \end{aligned}
\end{equation*}

\end{document}

