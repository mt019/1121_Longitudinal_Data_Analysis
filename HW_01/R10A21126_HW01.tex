\documentclass[UTF8,a4paper,10pt]{article}

\input{preamble.tex}

\begin{document}


\begin{mybox}{}
  Kernel estimator: 
  \begin{equation*}
    \begin{aligned}
      \omega(u,v)\triangleq \frac{1}{h}k(\frac{u-v}{h})
    \end{aligned}
  \end{equation*}
  with \(h>0\).
  


\end{mybox}


% Q1


  \begin{Problem}[]{}

    The local polynomial estimator can be derived as 
    \begin{equation*}
      \begin{aligned}
        (\tilde{\beta_0},\ldots,\tilde{\beta_k}) = \argmin_{\beta_0,\ldots,\beta_k} \frac{1}{N}\sum_{i=1}^{n}\sum_{j=1}^{m_i} (Y_{ij}-\sum_{j=0}^{k_i}\frac{g^{(j)}(t)}{j!}(t_{ij}-t)^{j})^2k_{h}(t-t_{ij}) 
      \end{aligned}
    \end{equation*}
    where
    \begin{equation*}
      \begin{aligned}
        k_{h}(v) = \frac{1}{h}k(\frac{v}{h}).
      \end{aligned}
    \end{equation*}
    It implies that \(\tilde{g}_h(t) = \tilde{\beta}_0\).

    Show that, when \(k=1\): (local linear fit)

    \begin{equation*}
      \begin{aligned}
        \tilde{g}_h(t) = \frac{\sum_{i=1}^{n}\sum_{j=1}^{m_i}\omega(t,t_{ij})Y_{ij}}{\sum_{i=1}^{n}\sum_{j=1}^{m_i}\omega(t,t_{ij})}
      \end{aligned}
    \end{equation*}
    where
    \begin{equation*}
      \begin{aligned}
        \omega(t,t_{ij}) = k_h(t-t_{ij})\{S_{n,2}+(t-t_{ij})S_{n,2}\},
      \end{aligned}
    \end{equation*}
    with
    \begin{equation*}
      \begin{aligned}
        S_{n,j} = \sum_{i=1}^{n}\sum_{j=1}^{m_i} k_h(t-t_{ij})(t-t_{ij})^j.
      \end{aligned}
    \end{equation*}
  \end{Problem}


  

  \begin{solution}\,\\

  \begin{equation*}
    \begin{aligned}
      Q &= \sum_{i=1}^{n}\sum_{j=1}^{m_i}  (Y_{ij}-\sum_{j=0}^{k_i}\frac{g^{(j)}(t)}{j!}(t_{ij}-t)^{j})^2k_{h}(t-t_{ij}) \\
      &= \sum_{i=1}^{n}\sum_{j=1}^{m_i} (Y_{ij}^2-\beta_0^2-2\beta_0Y_{ij}+\beta_1^2(t_{ij}-t)^2-2\beta_1Y_{ij}(t_{ij}-t)+2\beta_0\beta_1(t_{ij}-t))k_h(t-t_{ij})
    \end{aligned}
  \end{equation*}

  \begin{equation*}
    \begin{aligned}
      \frac{\partial Q}{\partial\beta_0} = \sum_{i=1}^{n}\sum_{j=1}^{m_i} (-2Y_{ij}-2\beta_0+2\beta_1(t_{ij}-t))k_h(t-t_{ij})\overset{\text{set}}{=} 0\\
      \frac{\partial Q}{\partial\beta_1} = \sum_{i=1}^{n}\sum_{j=1}^{m_i} (-2Y_{ij}(t_{ij}-t)+2\beta_1(t_{ij}-t)^2 +2\beta_0(t_{ij}-t))\overset{\text{set}}{=} 0
    \end{aligned}
  \end{equation*}

  \begin{equation*}
    \begin{aligned}
      \begin{pmatrix}
        S_{n,0} & S_{n,1}\\
        S_{n,1} & S_{n,2}
      \end{pmatrix}
    \begin{pmatrix}
      \tilde{\beta}_0\\
      \tilde{\beta}_1
    \end{pmatrix} 
    & =\\
      \begin{pmatrix}
        \sum\sum k_h(t-t_{ij}) & \sum\sum k_h(t-t_{ij})\\
        \sum\sum k_h(t-t_{ij})(t_{ij}-t) &  \sum\sum k_h(t-t_{ij})(t_{ij}-t)^2
      \end{pmatrix}
    \begin{pmatrix}
      \tilde{\beta}_0\\
      \tilde{\beta}_1
    \end{pmatrix}
    & =     
    \begin{pmatrix}
      \sum\sum Y_{ij}k_h(t-t_{ij})\\
      \sum\sum Y_{ij}k_h(t-t_{ij})(t_{ij}-t))
    \end{pmatrix}
    \end{aligned}
  \end{equation*}

  \begin{equation*}
    \begin{aligned}
      \begin{pmatrix}
        \tilde{\beta}_0\\
        \tilde{\beta}_1
      \end{pmatrix}
      & =       
      \begin{pmatrix}
        S_{n,0} & S_{n,1}\\
        S_{n,1} & S_{n,2}
      \end{pmatrix}^{-1}
      \begin{pmatrix}
        \sum\sum k_h(t-t_{ij})Y_{ij}
        \\
        \sum\sum k_h(t-t_{ij})(t_{ij}-t)Y_{ij}
      \end{pmatrix}\\
      & = \frac{1}{S_{n,0}S_{n,2}-S_{n,1}^2}
      \begin{pmatrix}
        S_{n,2} & -S_{n,1}\\
        -S_{n,1} & S_{n,0}
      \end{pmatrix}
      \begin{pmatrix}
        \sum\sum k_h(t-t_{ij})Y_{ij}
        \\
        \sum\sum k_h(t-t_{ij})(t_{ij}-t))Y_{ij}
      \end{pmatrix}\\
    \end{aligned}
  \end{equation*}


  Thus,

  \begin{equation*}
    \begin{aligned}
      \tilde{\beta}_0 & = \frac{S_{n,2}\sum\sum k_h(t-t_{ij})Y_{ij}-S_{n,1}\sum\sum k_h(t-t_{ij})(t_{ij}-t)Y_{ij}}{S_{n,0}S_{n,2}-S_{n,1}^2}\\
      % & = \frac{S_{n,2}\sum\sum k_h(t-t_{ij})Y_{ij}-S_{n,1}\sum\sum k_h(t-t_{ij})(t_{ij}-t)Y_{ij}}{S_{n,0}S_{n,2}-S_{n,1}^2}\\
      & = \frac{\sum\sum k_h(t-t_{ij})(S_{n,2}-S_{n,1}(t_{ij}-t))Y_{ij}}{\sum\sum k_h(t-t_{ij})\sum\sum k_h(t-t_{ij})(t_{ij}-t)^2- \sum\sum k_h(t-t_{ij})(t_{ij}-t)\sum\sum k_h(t-t_{ij})(t_{ij}-t)}\\
      & = \frac{\sum\sum k_h(t-t_{ij})\{S_{n,2}+(t-t_{ij})S_{n,1}\}Y_{ij}}{\sum\sum k_h(t-t_{ij})\sum\sum k_h(t-t_{ij})(t_{ij}-t)^2 + \sum\sum k_h(t-t_{ij})(t-t_{ij})\sum\sum k_h(t-t_{ij})(t_{ij}-t)}\\
      & = \frac{\sum\sum k_h(t-t_{ij})\{S_{n,2}+(t-t_{ij})S_{n,1}\}Y_{ij}}{\sum\sum k_h(t-t_{ij})\{S_{n,2}+(t-t_{ij})S_{n,1}\}}\\
      & = \frac{\sum\sum \omega (t,t_{ij})Y_{ij}}{\sum\sum \omega (t,t_{ij})}\\
    \end{aligned}
  \end{equation*}




  \end{solution}


  \begin{Problem}[]{}
    Explain
    \begin{enumerate}
      \item Total regression coefficient estimator
      \item Partial regression coefficient estimator
      \item Sequential regression coefficient estimator
    \end{enumerate}
    and when can they be the same.
  \end{Problem} 

  \begin{solution}

    The total regression coefficient estimator refers to the coefficients estimated in a regression model when you consider all the predictor variables simultaneously to predict the outcome variable.

    The partial regression coefficient estimator measures the expected change in the dependent variable associated with a one-unit change in an independent variable holding the other independent variables constant.
    
    Sequential regression coefficients refer to coefficients estimated in a stepwise regression procedure, which selects variables in a sequence based on certain criteria.

    The three estimators are the same in the simple linear regression model because there is only one dependent variable to be considered, thus the three estimators measure the relationship between the single independent variable and the dependent variable in the same way.
    
  \end{solution}

  \begin{Problem}[]{Time Series Correlation Structure}
The first-order autoregressive model with equally spaced time points:
\begin{equation*}
  \begin{aligned}
    \varepsilon_{i,j} = \rho\varepsilon_{i,j-1}+Z_{i,j}\text{ with }Z_{i,j}\overset{\text{iid}}{\sim } (0,(1-\rho^2)\sigma^2),|\rho|<1.
  \end{aligned}
\end{equation*}

Show that 
\begin{equation*}
  \begin{aligned}
    \Cov[\varepsilon_{i,j},\varepsilon_{i,j-k}] = \rho^k\sigma^2
  \end{aligned}
\end{equation*}

  \end{Problem} 

  \begin{solution}

    \begin{equation*}
      \begin{aligned}
        \Var[\varepsilon_{i,j}] &= \E[\varepsilon_{i,j}\varepsilon_{i,j} ]\\
        &= \E[(\rho \varepsilon_{i,j-1}+Z_{i,j})(\rho \varepsilon_{i,j-1}+Z_{i,j})]\\
        &=\E[\rho^2 \varepsilon_{i,j-1}^2 + 2\rho \varepsilon_{i,j-1}Z_{i,j} + Z_{i,j}^2]\\
        &=(1-\rho^2)\Var[\varepsilon_{i,j}] + 0 + \Var[Z_{i,j}]
      \end{aligned}
    \end{equation*}    
    
    \begin{equation*}
      \begin{aligned}
        \Var[\varepsilon_{i,j}] = \frac{\Var[Z_{i,j}]}{1-\rho^2} = \sigma^2
      \end{aligned}
    \end{equation*}

    \begin{equation*}
      \begin{aligned}
        \Cov[\varepsilon_{i,j},\varepsilon_{i,j-k}] &= \Cov[\rho \varepsilon_{i,j-1}+Z_{i,j},\varepsilon_{i,j-k}]\\
        & =\Cov[\rho \varepsilon_{i,j-1},\varepsilon_{i,j-k}]+\Cov[Z_{i,j},\varepsilon_{i,j-k}]\\
        & =\Cov[\rho \varepsilon_{i,j-1},\varepsilon_{i,j-k}] + 0\\
        & =\Cov[\rho^2 \varepsilon_{i,j-2} +\rho Z_{i,j-1},\varepsilon_{i,j-k}]\\
        & =\Cov[\rho^2 \varepsilon_{i,j-2},\varepsilon_{i,j-k}]\\
        &\vdots\\
        &=\Cov[\rho^k \varepsilon_{i,j-k},\varepsilon_{i,j-k}]\\
        &=\rho^k\Var[\varepsilon_{i,j-k}]\\
        &=\rho^k\sigma^2
      \end{aligned}
    \end{equation*}


  \end{solution}

  \begin{Problem}[]{Cross-Sectional Data}
    For Cross-Sectional Data
    \begin{equation*}
      \begin{aligned}
        Y = \X^T\beta + e, e \overset{\text{iid}}{\sim } (0,\sigma^2\mathbf{I}_n),
      \end{aligned}
    \end{equation*}
    prove that the least square estimator \(\hat{\beta} = (X^TX)^{-1}X^TY\) is the best linear unbiased estimator (BLUE) of \(\beta\).
    
      \end{Problem} 

    \begin{solution}

      To prove that the least square estimator \(\hat{\beta} = (X^TX)^{-1}X^TY\) is the best linear unbiased estimator (BLUE) of \(\beta\), is to show that
      \begin{enumerate}[label=(\alph*)]
        \item \(\E[\hat{\beta}|X] = \beta\),
        \item \(\Var[\tilde{\beta}|\X]-\Var[\hat{\beta}|\X]\succeq 0\),
      \end{enumerate}
      where \(\tilde{\beta} = A'X\) is an arbitrary linearly unbiased estimator for \(\beta\)





      Please show that under the assumption
      \begin{equation}
        \begin{aligned}
          \E[e|X] &= 0&&\text{No serial correlation}\\
          \E[ee'|\X]&=\sigma^2\mathbf{I}_n&&\text{Conditional homoskedasticity}
        \end{aligned}
      \end{equation}
      we have
      \begin{equation}
        \begin{aligned}
          \Var[\tilde{\beta}|\X]-\Var[\hat{\beta}|\X] =\sigma^2\A'\M\A.
        \end{aligned}
      \end{equation}










      Define
      \begin{equation}
        \begin{aligned}
          \A^{*\prime}:= (\X'\X)^{-1}\X'
        \end{aligned}
      \end{equation}
    The Least Square estimator 
      \begin{equation}
        \begin{aligned}
          \hat{\beta} &:= (\X'\X)^{-1}(\X'Y)\\
          &=\A^{*\prime}\Y
        \end{aligned}
      \end{equation}
      is the Best Linearly Unbiased Estimator (BLUE) for \(\beta\):
      \begin{equation}
        \begin{aligned}
          \Var[\tilde{\beta}|\X]\geq\Var[\hat{\beta}|\X]
        \end{aligned}
      \end{equation}
    where \(\tilde{\beta}\) is an arbitary linearly unbiased estimator for \(\beta\)
    \begin{equation}
      \begin{aligned}
        \tilde{\beta}=\A^{\prime}\Y
      \end{aligned}
    \end{equation}
    with
    \begin{equation}
      \begin{aligned}
        \E[\tilde{\beta}|\X]&=\E[\A^{\prime}\Y|\X]\\
        &=\E[\A^{\prime}(\X\beta+e)|\X]\\
        &=\A^{\prime}\X\beta\\
        &=\beta,
      \end{aligned}
    \end{equation}
    which implies that
    \begin{equation}
      \begin{aligned}
        \A^{\prime}\X&=\mathbf{I}_k.
      \end{aligned}
    \end{equation}
    If \(\A = \A^*\), we have \((\X'\X)^{-1}\X'\X = \mathbf{I}_k\).
    
    \begin{equation}
      \begin{aligned}
        \Var[\tilde{\beta}|\X]&=\E[(\tilde{\beta}-\E[\tilde{\beta}|\X])(\tilde{\beta}-\E[\tilde{\beta}|\X])'|\X]\\
        &=\E[(\tilde{\beta}-\beta)(\tilde{\beta}-\beta)'|\X]\\
      \end{aligned}
    \end{equation}
    \begin{equation}
      \begin{aligned}
        \Var[\tilde{\beta}|\X]&=\Var[\A^{\prime}\Y|\X]\\
        &=\Var[\A^{\prime}e|\X]\\
        &=\A^{\prime}\Var[e|\X]\A\\
        &=\A^{\prime}\E[ee'|\X]\A\\
        &=\A^{\prime}\sigma^2\mathbf{I}_n\A\\
        &=\sigma^2\A^{\prime}\A
      \end{aligned}
    \end{equation}
    \begin{equation}
      \begin{aligned}
        \Var[\hat{\beta}|\X]&=\Var[\A^{*\prime}\Y|\X]\\
        &=\Var[\A^{*\prime}e|\X]\\
        &=\A^{*\prime}\Var[e|\X]\A^*\\
        &=\A^{*\prime}\E[ee'|\X]\A^*\\
        &=\A^{*\prime}\sigma^2\mathbf{I}_n\A^*\\
        &=\sigma^2\A^{*\prime}\A^*
      \end{aligned}
    \end{equation}
    \begin{equation}
      \begin{aligned}
        \Var[\tilde{\beta}|\X]-\Var[\hat{\beta}|\X]&=\sigma^2\A^{\prime}\A-\sigma^2\A^{*\prime}\A^*\\
        &=\sigma^2(\A^{\prime}\A-\A^{*\prime}\A^*)\\
        &=\sigma^2(\A^{\prime}\A-(\X'\X)^{-1}\X'((\X'\X)^{-1}\X')')\\
        &=\sigma^2(\A^{\prime}\A-(\X'\X)^{-1}\X'\X(\X'\X)^{-1})\\
        &=\sigma^2(\A^{\prime}\A-(\X'\X)^{-1})\\
        &=\sigma^2(\A'\A-\mathbf{I}_k(\X'\X)^{-1}\mathbf{I}_k)\\
        &=\sigma^2(\A'\A-\A'\X(\X'\X)^{-1}\X'\A)\\
        &=\sigma^2(\A'[\mathbf{I}_n-\X(\X'\X)^{-1}\X']\A)\\
        &=\sigma^2\A'\M\A
      \end{aligned}
    \end{equation}



    \end{solution}
    
  \begin{Problem}[]{}

    GLM: \(Y = X\beta +\varepsilon \), where \(\varepsilon \sim(0,\sigma^2V)\).
    Let \(A = I-X(X^TX)^{-1}X^T\) and \(B\) be the \(N\times (N-(p-1))\) matrix such that \(BB^T = A\) and \(B^TB = I_{N-(p-1)}\).

    For any fixed \(V\), the MLE of \(\beta\) is \(\hat{\beta}(V) = GY\), with \(G = (X^TV^{-1}X)^{-1}X^TV^{-1}\)
    Show that
    \begin{equation*}
      \begin{aligned}
        |GG^T-GBB^TG^T|^{\frac{1}{2}} = |X^TX|^{-\frac{1}{2}}
      \end{aligned}
    \end{equation*}
    
      \end{Problem} 


          
  \begin{Problem}[]{Serial rorrelation plus mesurement error}
    Mesurement error: \(\varepsilon \sim (0,\sigma^2 H+\tau^2I_N)\).

    Show that the corresponding variogram is \(r(u) = \tau^2+\sigma^2(1-\rho(u))\) for \(u\geq 0\), with \(r(0) = \tau^2>0\).

    
      \end{Problem} 


  \begin{Problem}[]{Repeated Measures}

    \begin{equation*}
      \begin{aligned}
        y_{hij} = \beta_{h}+r_{hj}+ U_{hi}+z_{hij}, h = 1,\ldots,g, i=1,\ldots,n_h, j=1,\ldots,m,
      \end{aligned}
    \end{equation*}
    where \(\beta_h\) is the main effect for the \(h\)-th treatment, and \(r_{hj}\) is the interaction effect between the \(h\)-th treatment and the \(j\)-th time with \(\sum_{j=1}^{m}r_{hj} = 0 ,\forall h\).

    For the hypothesis \(H_0:\beta_1 = \cdots\beta_g\)
find the F statistics \(F = \frac{MBTSS_1}{MRSS_1}\) of the ANOVA table.

    
  \end{Problem} 

  
  \begin{solution}\,\\


  \end{solution}
    

\end{document}