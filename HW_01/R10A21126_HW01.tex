\documentclass[UTF8,a4paper,10pt]{article}

\input{preamble.tex}

\begin{document}


\begin{mybox}{}
  Kernel estimator: 
  \begin{equation*}
    \begin{aligned}
      \omega(u,v)\triangleq \frac{1}{h}k(\frac{u-v}{h})
    \end{aligned}
  \end{equation*}
  with \(h>0\).
  


\end{mybox}


% Q1


  \begin{Problem}[]{}

    The local polynomial estimator can be derived as 
    \begin{equation*}
      \begin{aligned}
        (\tilde{\beta_0},\ldots,\tilde{\beta_k}) = \argmin_{\beta_0,\ldots,\beta_k} \frac{1}{N}\sum_{i=1}^{n}\sum_{j=1}^{m_i} (Y_{ij}-\sum_{j=0}^{k_i}\frac{g^{(j)}(t)}{j!}(t_{ij}-t)^{j})^2k_{h}(t-t_{ij}) 
      \end{aligned}
    \end{equation*}
    where
    \begin{equation*}
      \begin{aligned}
        k_{h}(v) = \frac{1}{h}k(\frac{v}{h}).
      \end{aligned}
    \end{equation*}
    It implies that \(\tilde{g}_h(t) = \tilde{\beta}_0\).

    Show that, when \(k=1\): (local linear fit)

    \begin{equation*}
      \begin{aligned}
        \tilde{g}_h(t) = \frac{\sum_{i=1}^{n}\sum_{j=1}^{m_i}\omega(t,t_{ij})Y_{ij}}{\sum_{i=1}^{n}\sum_{j=1}^{m_i}\omega(t,t_{ij})}
      \end{aligned}
    \end{equation*}
    where
    \begin{equation*}
      \begin{aligned}
        \omega(t,t_{ij}) = k_h(t-t_{ij})\{S_{n,2}+(t-t_{ij})S_{n,2}\},
      \end{aligned}
    \end{equation*}
    with
    \begin{equation*}
      \begin{aligned}
        S_{n,j} = \sum_{i=1}^{n}\sum_{j=1}^{m_i} k_h(t-t_{ij})(t-t_{ij})^j.
      \end{aligned}
    \end{equation*}
  \end{Problem}


  

  \begin{solution}\,\\

  \begin{equation*}
    \begin{aligned}
      Q &= \sum_{i=1}^{n}\sum_{j=1}^{m_i}  (Y_{ij}-\sum_{j=0}^{k_i}\frac{g^{(j)}(t)}{j!}(t_{ij}-t)^{j})^2k_{h}(t-t_{ij}) \\
      &= \sum_{i=1}^{n}\sum_{j=1}^{m_i} (Y_{ij}^2-\beta_0^2-2\beta_0Y_{ij}+\beta_1^2(t_{ij}-t)^2-2\beta_1Y_{ij}(t_{ij}-t)+2\beta_0\beta_1(t_{ij}-t))k_h(t-t_{ij})
    \end{aligned}
  \end{equation*}

  \begin{equation*}
    \begin{aligned}
      \frac{\partial Q}{\partial\beta_0} = \sum_{i=1}^{n}\sum_{j=1}^{m_i} (-2Y_{ij}-2\beta_0+2\beta_1(t_{ij}-t))k_h(t-t_{ij})\overset{\text{set}}{=} 0\\
      \frac{\partial Q}{\partial\beta_1} = \sum_{i=1}^{n}\sum_{j=1}^{m_i} (-2Y_{ij}(t_{ij}-t)+2\beta_1(t_{ij}-t)^2 +2\beta_0(t_{ij}-t))\overset{\text{set}}{=} 0
    \end{aligned}
  \end{equation*}

  \begin{equation*}
    \begin{aligned}
      \begin{pmatrix}
        S_{n,0} & S_{n,1}\\
        S_{n,1} & S_{n,2}
      \end{pmatrix}
    \begin{pmatrix}
      \tilde{\beta}_0\\
      \tilde{\beta}_1
    \end{pmatrix} 
    & =\\
      \begin{pmatrix}
        \sum\sum k_h(t-t_{ij}) & \sum\sum k_h(t-t_{ij})\\
        \sum\sum k_h(t-t_{ij})(t_{ij}-t) &  \sum\sum k_h(t-t_{ij})(t_{ij}-t)^2
      \end{pmatrix}
    \begin{pmatrix}
      \tilde{\beta}_0\\
      \tilde{\beta}_1
    \end{pmatrix}
    & =     
    \begin{pmatrix}
      \sum\sum Y_{ij}k_h(t-t_{ij})\\
      \sum\sum Y_{ij}k_h(t-t_{ij})(t_{ij}-t))
    \end{pmatrix}
    \end{aligned}
  \end{equation*}

  \begin{equation*}
    \begin{aligned}
      \begin{pmatrix}
        \tilde{\beta}_0\\
        \tilde{\beta}_1
      \end{pmatrix}
      & =       
      \begin{pmatrix}
        S_{n,0} & S_{n,1}\\
        S_{n,1} & S_{n,2}
      \end{pmatrix}^{-1}
      \begin{pmatrix}
        \sum\sum k_h(t-t_{ij})Y_{ij}
        \\
        \sum\sum k_h(t-t_{ij})(t_{ij}-t)Y_{ij}
      \end{pmatrix}\\
      & = \frac{1}{S_{n,0}S_{n,2}-S_{n,1}^2}
      \begin{pmatrix}
        S_{n,2} & -S_{n,1}\\
        -S_{n,1} & S_{n,0}
      \end{pmatrix}
      \begin{pmatrix}
        \sum\sum k_h(t-t_{ij})Y_{ij}
        \\
        \sum\sum k_h(t-t_{ij})(t_{ij}-t))Y_{ij}
      \end{pmatrix}\\
    \end{aligned}
  \end{equation*}


  Thus,

  \begin{equation*}
    \begin{aligned}
      \tilde{\beta}_0 & = \frac{S_{n,2}\sum\sum k_h(t-t_{ij})Y_{ij}-S_{n,1}\sum\sum k_h(t-t_{ij})(t_{ij}-t)Y_{ij}}{S_{n,0}S_{n,2}-S_{n,1}^2}\\
      % & = \frac{S_{n,2}\sum\sum k_h(t-t_{ij})Y_{ij}-S_{n,1}\sum\sum k_h(t-t_{ij})(t_{ij}-t)Y_{ij}}{S_{n,0}S_{n,2}-S_{n,1}^2}\\
      & = \frac{\sum\sum k_h(t-t_{ij})(S_{n,2}-S_{n,1}(t_{ij}-t))Y_{ij}}{\sum\sum k_h(t-t_{ij})\sum\sum k_h(t-t_{ij})(t_{ij}-t)^2- \sum\sum k_h(t-t_{ij})(t_{ij}-t)\sum\sum k_h(t-t_{ij})(t_{ij}-t)}\\
      & = \frac{\sum\sum k_h(t-t_{ij})\{S_{n,2}+(t-t_{ij})S_{n,1}\}Y_{ij}}{\sum\sum k_h(t-t_{ij})\sum\sum k_h(t-t_{ij})(t_{ij}-t)^2 + \sum\sum k_h(t-t_{ij})(t-t_{ij})\sum\sum k_h(t-t_{ij})(t_{ij}-t)}\\
      & = \frac{\sum\sum k_h(t-t_{ij})\{S_{n,2}+(t-t_{ij})S_{n,1}\}Y_{ij}}{\sum\sum k_h(t-t_{ij})\{S_{n,2}+(t-t_{ij})S_{n,1}\}}\\
      & = \frac{\sum\sum \omega (t,t_{ij})Y_{ij}}{\sum\sum \omega (t,t_{ij})}\\
    \end{aligned}
  \end{equation*}




  \end{solution}


  \begin{Problem}[]{}
    Explain
    \begin{enumerate}
      \item Total regression coefficient estimator
      \item Partial regression coefficient estimator
      \item Sequential regression coefficient estimator
    \end{enumerate}
    and when can they be the same.
  \end{Problem} 

  \begin{solution}

    The total regression coefficient estimator refers to the coefficients estimated in a regression model when you consider all the predictor variables simultaneously to predict the outcome variable.

    The partial regression coefficient estimator measures the expected change in the dependent variable associated with a one-unit change in an independent variable holding the other independent variables constant.
    
    Sequential regression coefficients refer to coefficients estimated in a stepwise regression procedure, which selects variables in a sequence based on certain criteria.

    The three estimators are the same in the simple linear regression model because there is only one dependent variable to be considered, thus the three estimators measure the relationship between the single independent variable and the dependent variable in the same way.
    
  \end{solution}

  \begin{Problem}[]{Time Series Correlation Structure}
The first-order autoregressive model with equally spaced time points:
\begin{equation*}
  \begin{aligned}
    \varepsilon_{i,j} = \rho\varepsilon_{i,j-1}+Z_{i,j}\text{ with }Z_{i,j}\overset{\text{iid}}{\sim } (0,(1-\rho^2)\sigma^2),|\rho|<1.
  \end{aligned}
\end{equation*}

Show that 
\begin{equation*}
  \begin{aligned}
    \Cov[\varepsilon_{i,j},\varepsilon_{i,j-k}] = \rho^k\sigma^2
  \end{aligned}
\end{equation*}

  \end{Problem} 

  \begin{solution}

    \begin{equation*}
      \begin{aligned}
        \Var[\varepsilon_{i,j}] &= \E[\varepsilon_{i,j}\varepsilon_{i,j} ]\\
        &= \E[(\rho \varepsilon_{i,j-1}+Z_{i,j})(\rho \varepsilon_{i,j-1}+Z_{i,j})]\\
        &=\E[\rho^2 \varepsilon_{i,j-1}^2 + 2\rho \varepsilon_{i,j-1}Z_{i,j} + Z_{i,j}^2]\\
        &=(1-\rho^2)\Var[\varepsilon_{i,j}] + 0 + \Var[Z_{i,j}]
      \end{aligned}
    \end{equation*}    
    
    \begin{equation*}
      \begin{aligned}
        \Var[\varepsilon_{i,j}] = \frac{\Var[Z_{i,j}]}{1-\rho^2} = \sigma^2
      \end{aligned}
    \end{equation*}

    \begin{equation*}
      \begin{aligned}
        \Cov[\varepsilon_{i,j},\varepsilon_{i,j-k}] &= \Cov[\rho \varepsilon_{i,j-1}+Z_{i,j},\varepsilon_{i,j-k}]\\
        & =\Cov[\rho \varepsilon_{i,j-1},\varepsilon_{i,j-k}]+\Cov[Z_{i,j},\varepsilon_{i,j-k}]\\
        & =\Cov[\rho \varepsilon_{i,j-1},\varepsilon_{i,j-k}] + 0\\
        & =\Cov[\rho^2 \varepsilon_{i,j-2} +\rho Z_{i,j-1},\varepsilon_{i,j-k}]\\
        & =\Cov[\rho^2 \varepsilon_{i,j-2},\varepsilon_{i,j-k}]\\
        &\vdots\\
        &=\Cov[\rho^k \varepsilon_{i,j-k},\varepsilon_{i,j-k}]\\
        &=\rho^k\Var[\varepsilon_{i,j-k}]\\
        &=\rho^k\sigma^2
      \end{aligned}
    \end{equation*}


  \end{solution}
\pagebreak

% Q4

  \begin{Problem}[]{Cross-Sectional Data}
    For Cross-Sectional Data
    \begin{equation*}
      \begin{aligned}
        Y = \X'\beta + e, e \overset{\text{iid}}{\sim } (0,\sigma^2\mathbf{I}_n),
      \end{aligned}
    \end{equation*}
    prove that the least square estimator \(\hat{\beta} = (\X'\X)^{-1}\X'Y\) is the best linear unbiased estimator (BLUE) of \(\beta\).
    
      \end{Problem} 

    \begin{solution}

      To prove that the least square estimator \(\hat{\beta} = (\X'\X)^{-1}\X'Y\) is the best linear unbiased estimator (BLUE) of \(\beta\), is to show that
      \begin{enumerate}[label=(\alph*)]
        \item \(\E[\hat{\beta}|\X] = \beta\),
        \item \(\Var[\tilde{\beta}|\X]-\Var[\hat{\beta}|\X]\succeq 0\),
      \end{enumerate}
      where \(\tilde{\beta} = \A'\X\) is an arbitrary linearly unbiased estimator for \(\beta\).

      \dotfill

      % Please show that under the assumption
      % \begin{equation*}
      %   \begin{aligned}
      %     \E[e|X] &= 0&&\text{No serial correlation}\\
      %     \E[ee'|\X]&=\sigma^2\mathbf{I}_n&&\text{Conditional homoskedasticity}
      %   \end{aligned}
      % \end{equation*}
      % we have
      % \begin{equation*}
      %   \begin{aligned}
      %     \Var[\tilde{\beta}|\X]-\Var[\hat{\beta}|\X] =\sigma^2\A'\M\A.
      %   \end{aligned}
      % \end{equation*}


      Define
      \begin{equation*}
        \begin{aligned}
          \A^{*\prime}:= (\X'\X)^{-1}\X'.
        \end{aligned}
      \end{equation*}
    The Least Square estimator 
      \begin{equation*}
        \begin{aligned}
          \hat{\beta} &:= (\X'\X)^{-1}(\X'Y)\\
          &=\A^{*\prime}Y.
        \end{aligned}
      \end{equation*}
      % is the Best Linearly Unbiased Estimator (BLUE) for \(\beta\):
      % \begin{equation*}
      %   \begin{aligned}
      %     \Var[\tilde{\beta}|\X]\geq\Var[\hat{\beta}|\X]
      %   \end{aligned}
      % \end{equation*}


    Let \(\tilde{\beta}\) be an arbitrary linearly unbiased estimator for \(\beta\)
    \begin{equation*}
      \begin{aligned}
        \tilde{\beta}=\A^{\prime}Y,
      \end{aligned}
    \end{equation*}
    with
    \begin{equation*}
      \begin{aligned}
        \E[\tilde{\beta}|\X]&=\E[\A^{\prime}Y|\X]\\
        &=\E[\A^{\prime}(\X\beta+e)|\X]\\
        &=\A^{\prime}\X\beta\\
        &=\beta,
      \end{aligned}
    \end{equation*}
    which requires that
    \begin{equation*}
      \begin{aligned}
        \A^{\prime}\X&=\mathbf{I}_k.
      \end{aligned}
    \end{equation*}
    If \(\A = \A^*\), we have \((\X'\X)^{-1}\X'\X = \mathbf{I}_k\). Thus the LSE \(\hat{\beta}\) is unbiased.
    
    \begin{equation*}
      \begin{aligned}
        \Var[\tilde{\beta}|\X]&=\E[(\tilde{\beta}-\E[\tilde{\beta}|\X])(\tilde{\beta}-\E[\tilde{\beta}|\X])'|\X]\\
        &=\E[(\tilde{\beta}-\beta)(\tilde{\beta}-\beta)'|\X]\\
      \end{aligned}
    \end{equation*}
    \begin{equation*}
      \begin{aligned}
        \Var[\tilde{\beta}|\X]&=\Var[\A^{\prime}Y|\X]\\
        &=\Var[\A^{\prime}e|\X]\\
        &=\A^{\prime}\Var[e|\X]\A\\
        % &=\A^{\prime}\E[ee'|\X]\A\\
        &=\A^{\prime}\sigma^2\mathbf{I}_n\A\\
        &=\sigma^2\A^{\prime}\A
      \end{aligned}
    \end{equation*}
    \begin{equation*}
      \begin{aligned}
        \Var[\hat{\beta}|\X]&=\Var[\A^{*\prime}Y|\X]\\
        &=\Var[\A^{*\prime}e|\X]\\
        &=\A^{*\prime}\Var[e|\X]\A^*\\
        &=\A^{*\prime}\E[ee'|\X]\A^*\\
        &=\A^{*\prime}\sigma^2\mathbf{I}_n\A^*\\
        &=\sigma^2\A^{*\prime}\A^*
      \end{aligned}
    \end{equation*}
    \begin{equation*}
      \begin{aligned}
        \Var[\tilde{\beta}|\X]-\Var[\hat{\beta}|\X]&=\sigma^2\A^{\prime}\A-\sigma^2\A^{*\prime}\A^*\\
        &=\sigma^2(\A^{\prime}\A-\A^{*\prime}\A^*)\\
        &=\sigma^2(\A^{\prime}\A-(\X'\X)^{-1}\X'((\X'\X)^{-1}\X')')\\
        &=\sigma^2(\A^{\prime}\A-(\X'\X)^{-1}\X'\X(\X'\X)^{-1})\\
        &=\sigma^2(\A^{\prime}\A-(\X'\X)^{-1})\\
        &=\sigma^2(\A'\A-\mathbf{I}_k(\X'\X)^{-1}\mathbf{I}_k)\\
        &=\sigma^2(\A'\A-\A'\X(\X'\X)^{-1}\X'\A)\\
        &=\sigma^2(\A'[\mathbf{I}_n-\X(\X'\X)^{-1}\X']\A)\\
        &=\sigma^2\A'\M\A.
      \end{aligned}
    \end{equation*}
    For any nonzero vector \(\lambda\), we have
    \begin{equation*}
      \begin{aligned}
        \lambda' (\Var[\tilde{\beta}|\X]-\Var[\hat{\beta}|\X]) \lambda 
        &=  \sigma^2 \lambda' \A'\M\A \lambda\\
        &= \sigma^2 \lambda' \A'\M\M\A \lambda\\
        &= \sigma^2(\M\A \lambda)' (\M\A \lambda)\geq 0,
      \end{aligned}
    \end{equation*}
    implying that \(\Var[\tilde{\beta}|\X]-\Var[\hat{\beta}|\X]\) is positive semi-definite.

    Therefore, we can conclude that the least square estimator \(\hat{\beta} = (\X'\X)^{-1}\X'Y\) is the best linear unbiased estimator (BLUE) of \(\beta\).



    \begin{mybox}{}
      Projection Matrix:
      \[\mathbf{P}:=\X{(\X'\X)}^{-1}\X'\]
      Orthogonal Projection Matrix:
      \[\M:=\mathbf{I}_n-\mathbf{P} =\mathbf{I}_n- \X{(\X'\X)}^{-1}\X'\]
      We can show that $\mathbf{P}$ and $\mathbf{M}$ are both symmetric and idempotent. That is 
      \begin{align*}
          \mathbf{P}' &= \mathbf{P}\\
          \mathbf{P}\mathbf{P} &= \mathbf{P}\\
          \M' &= \M\\
          \M\M &= \M
      \end{align*}

      \dotfill
    \begin{multicols*}{2}
      \begin{align*}
        \mathbf{P}' &=[\X{(\X'\X)}^{-1}\X']'\\
        & = [\X']'[\X{(\X'\X)}^{-1}]'\\
        & = \X[{(\X'\X)}^{-1}]'[\X]'\\
        & = \X{[(\X'\X)']}^{-1}[\X]'\\
        & = \X{[\X'\X]}^{-1}[\X]'\\
        &= \mathbf{P} \quad\text{symmetric}
\\[6pt]
        \mathbf{P}\mathbf{P} &=\underset{\X}{\underbrace{\mathbf{P}\X}}{(\X'\X)}^{-1}\X'\\
        &=\X{(\X'\X)}^{-1}\X'
        % \quad\text{Property of a projection matrix}
        \\
        &= \mathbf{P}\quad\text{idempotent}
      \end{align*}

      \begin{align*}
        \M' &=[\mathbf{I}_n-\mathbf{P}]'\\
        & = [\mathbf{I}_n]'-[\mathbf{P}]'\\
        & = \mathbf{I}_n-\mathbf{P}\\
        & = \M\quad\text{symmetric}
\\[6pt]
        \M\M &=[\mathbf{I}_n-\mathbf{P}][\mathbf{I}_n-\mathbf{P}]\\
        & = [\mathbf{I}_n-\mathbf{P}]\mathbf{I}_n-[\mathbf{I}_n-\mathbf{P}]\mathbf{P}\\
        &=\mathbf{I}_n-\mathbf{P}-\mathbf{P}+\mathbf{P}\\
        &=\mathbf{I}_n-\mathbf{P}\\
        &=\M \quad\text{idempotent}
    \end{align*}

  \end{multicols*}

  \end{mybox}



    \end{solution}
    
  \begin{Problem}[]{}

    GLM: \(Y = X\beta +\varepsilon \), where \(\varepsilon \sim(0,\sigma^2V)\).
    Let \(A = I-X(X^{T}X)^{-1}X^{T}\) and \(B\) be the \(N\times (N-(p+1))\) matrix such that \(BB^{T} = A\) and \(B^{T}B = I_{N-(p+1)}\).

    For any fixed \(V\), the MLE of \(\beta\) is \(\hat{\beta}(V) = GY\), with \(G = (X^{T}V^{-1}X)^{-1}X^{T}V^{-1}\)
    Show that
    \begin{equation*}
      \begin{aligned}
        |GG^{T}-GBB^{T}G^{T}|^{\frac{1}{2}} = |X^{T}X|^{-\frac{1}{2}}
      \end{aligned}
    \end{equation*}
    
      \end{Problem} 
  
  \begin{solution}\,\\

    \begin{equation*}
      \begin{aligned}
        GG^{T}-GBB^{T}G^{T}
        &= GG^{T}-GAG^{T}\\
        &= GG^{T}-G[I-X(X^{T}X)^{-1}X^{T}]G^{T}\\
        &= GG^{T}-GG^{T} + GX(X^{T}X)^{-1}X^{T}G^{T}\\
        &= GX(X^{T}X)^{-1}X^{T}G^{T}
      \end{aligned}
    \end{equation*}
    \begin{equation*}
      \begin{aligned}
        |GG^{T}-GBB^{T}G^{T}|^{\frac{1}{2}} 
        &= |GX(X^{T}X)^{-1}X^{T}G^{T}|^{\frac{1}{2}} \\
        &= |G|^{\frac{1}{2}} |X(X^{T}X)^{-1}X^{T}|^{\frac{1}{2}} |G^{T}|^{\frac{1}{2}} \\
        &= |G||X(X^{T}X)^{-1}X^{T}|^{\frac{1}{2}} \\
        &= |(X^{T}V^{-1}X)^{-1}X^{T}V^{-1}|  |X|^{\frac{1}{2}}|(X^{T}X)^{-1}|^{\frac{1}{2}}|X^{T}|^{\frac{1}{2}}\\
        &=\frac{1}{|X^{T}V^{-1}X|}|X^{T}V^{-1}||X|^{\frac{1}{2}}|(X^{T}X)^{-1}|^{\frac{1}{2}}|X^{T}|^{\frac{1}{2}}\\
        &=|(X^{T}X)^{-1}|^{\frac{1}{2}}\\
        &=|X^{T}X|^{-\frac{1}{2}}
      \end{aligned}
    \end{equation*}
  \end{solution}
    

          
  \begin{Problem}[]{Serial correlation plus mesurement error}

    When \(\varepsilon(t) \) is stationary, the covariance function or variogram :
    \begin{equation*}
      \begin{aligned}
        r(u) = \frac{1}{2}\E[(\varepsilon(t)-\varepsilon(t-u))^2].
      \end{aligned}
    \end{equation*}

Sources of random variation in longitudinal data:

    Serial correlation + Mesurement error mesurement error: \(\varepsilon \sim (0,\sigma^2 H+\tau^2I_N)\).

    Show that the corresponding variogram is \(r(u) = \tau^2+\sigma^2(1-\rho(u))\) for \(u\geq 0\), with \(r(0) = \tau^2>0\).

    
      \end{Problem} 


      \begin{solution}\,
        \begin{equation*}
          \begin{aligned}
            r(u) 
            &= \frac{1}{2}\E[(\varepsilon(t)-\varepsilon(t-u))^2]\\
            &= \frac{1}{2}\E[\varepsilon(t)^2] +\frac{1}{2}\E[\varepsilon(t-u)^2]-\E[\varepsilon(t)\varepsilon(t-u)]\\
            &= \sigma^2+\tau^2 -\sigma^2\rho(u)\\
            &= \sigma^2(1-\rho(u))+\tau^2,
          \end{aligned}
        \end{equation*}
        where \(\rho(0) = 1\).

      \end{solution}

  \begin{Problem}[]{Repeated Measures}

    \begin{equation*}
      \begin{aligned}
        y_{hij} = \beta_{h}+r_{hj}+ U_{hi}+z_{hij}, h = 1,\ldots,g, i=1,\ldots,n_h, j=1,\ldots,m,
      \end{aligned}
    \end{equation*}
    where \(\beta_h\) is the main effect for the \(h\)-th treatment, and \(r_{hj}\) is the interaction effect between the \(h\)-th treatment and the \(j\)-th time with \(\sum_{j=1}^{m}r_{hj} = 0 ,\forall h\).

    For the hypothesis \(H_0:\beta_1 = \cdots\beta_g\)
find the F statistics of the ANOVA table.

    
  \end{Problem} 

  
  \begin{solution}\,

    \begin{table}[htbp]
      % \centering
      \caption{ANOVA Table}
      \hspace{-1cm}
      \begin{tabular}{lllc}
          \toprule
          \textbf{Source of Variation} & \textbf{Sum of Squares (SS)} & \textbf{Degrees of Freedom (df)} & \textbf{Mean Square (MS)} \\
          \midrule
          Between Treatments & SSB = \(\sum_{h=1}^{g}(n_h m)(\bar{y}_{h\cdot\cdot}-\bar{y}_{\cdot\cdot\cdot})^2\) & df$_{\text{between}}$  = \(g-1\) & MSB = SSB/df$_{\text{between}}$ \\
          Within Treatments & SSW = \(\sum_{h=1}^{g}\sum_{i=1}^{n_h}m(\bar{y}_{hi\cdot}-\bar{y}_{h\cdot\cdot})^2\) & df$_{\text{within}}$  = \(\sum_{h=1}^{g}n_h -g\) & MSW = SSW/df$_{\text{within}}$ \\
          Total & SST = \(\sum_{h=1}^{g}\sum_{i=1}^{n_h}m(\bar{y}_{hi\cdot}-\bar{y}_{\cdot\cdot\cdot})^2\)  & df$_{\text{total}}$ = \(\sum_{h=1}^{g}n_h -1\) & \\
          \bottomrule
      \end{tabular}
  \end{table}
  
  \begin{table}[htbp]
      \centering
      \caption{F-statistic}
      \begin{tabular}{c}
          \textbf{F-statistic} \\
          \midrule
          F = MSB/MSW \\
          \bottomrule
      \end{tabular}
  \end{table}
  

% \begin{landscape}
%   \begin{table}
%       \centering
%       \caption{ANOVA Table}
%       \small % Adjust the font size to make the table smaller
%       \begin{adjustbox}{width=1.3\textwidth} % Adjust the width of the table
%       \begin{tabular}{lcccc}
%           \toprule
%           \textbf{Source of Variation} & \textbf{Sum of Squares (SS)} & \textbf{Degrees of Freedom (df)} & \textbf{Mean Square (MS)} & \textbf{F-statistic} \\
%           \midrule
%           Between Treatments & SSB & df$_{\text{between}}$ & MSB = SSB/df$_{\text{between}}$ & F = MSB/MSW \\
%           Within Treatments & SSW & df$_{\text{within}}$ & MSW = SSW/df$_{\text{within}}$ & \\
%           Total & SST & df$_{\text{total}}$ & & \\
%           \bottomrule
%       \end{tabular}
%       \end{adjustbox}
%   \end{table}
%   \end{landscape}


  \end{solution}
    

\end{document}